{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 1080,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04640371229698376,
      "grad_norm": 11.419303894042969,
      "learning_rate": 0.00012,
      "loss": 3.9599,
      "step": 10
    },
    {
      "epoch": 0.09280742459396751,
      "grad_norm": 1.1645411252975464,
      "learning_rate": 0.00019906250000000002,
      "loss": 3.0152,
      "step": 20
    },
    {
      "epoch": 0.13921113689095127,
      "grad_norm": 0.8644851446151733,
      "learning_rate": 0.00019750000000000003,
      "loss": 2.6102,
      "step": 30
    },
    {
      "epoch": 0.18561484918793503,
      "grad_norm": 0.7908639907836914,
      "learning_rate": 0.0001959375,
      "loss": 2.5049,
      "step": 40
    },
    {
      "epoch": 0.23201856148491878,
      "grad_norm": 0.8473855257034302,
      "learning_rate": 0.00019437500000000002,
      "loss": 2.4916,
      "step": 50
    },
    {
      "epoch": 0.27842227378190254,
      "grad_norm": 0.8470242023468018,
      "learning_rate": 0.00019281250000000003,
      "loss": 2.3871,
      "step": 60
    },
    {
      "epoch": 0.3248259860788863,
      "grad_norm": 0.8411765694618225,
      "learning_rate": 0.00019125000000000001,
      "loss": 2.4221,
      "step": 70
    },
    {
      "epoch": 0.37122969837587005,
      "grad_norm": 1.5146583318710327,
      "learning_rate": 0.00018968750000000002,
      "loss": 2.3398,
      "step": 80
    },
    {
      "epoch": 0.4176334106728538,
      "grad_norm": 0.8721016645431519,
      "learning_rate": 0.000188125,
      "loss": 2.2101,
      "step": 90
    },
    {
      "epoch": 0.46403712296983757,
      "grad_norm": 0.848953366279602,
      "learning_rate": 0.00018656250000000001,
      "loss": 2.2779,
      "step": 100
    },
    {
      "epoch": 0.5104408352668214,
      "grad_norm": 0.8207142949104309,
      "learning_rate": 0.00018500000000000002,
      "loss": 2.3222,
      "step": 110
    },
    {
      "epoch": 0.5568445475638051,
      "grad_norm": 0.8610983490943909,
      "learning_rate": 0.0001834375,
      "loss": 2.2532,
      "step": 120
    },
    {
      "epoch": 0.6032482598607889,
      "grad_norm": 0.9528675675392151,
      "learning_rate": 0.00018187500000000002,
      "loss": 2.1497,
      "step": 130
    },
    {
      "epoch": 0.6496519721577726,
      "grad_norm": 1.0417040586471558,
      "learning_rate": 0.00018031250000000003,
      "loss": 2.2309,
      "step": 140
    },
    {
      "epoch": 0.6960556844547564,
      "grad_norm": 0.857600748538971,
      "learning_rate": 0.00017875,
      "loss": 2.0674,
      "step": 150
    },
    {
      "epoch": 0.7424593967517401,
      "grad_norm": 0.8294448852539062,
      "learning_rate": 0.00017718750000000002,
      "loss": 2.2196,
      "step": 160
    },
    {
      "epoch": 0.7888631090487239,
      "grad_norm": 0.844781756401062,
      "learning_rate": 0.00017562500000000003,
      "loss": 2.0797,
      "step": 170
    },
    {
      "epoch": 0.8352668213457076,
      "grad_norm": 0.9279717206954956,
      "learning_rate": 0.0001740625,
      "loss": 2.1464,
      "step": 180
    },
    {
      "epoch": 0.8816705336426914,
      "grad_norm": 0.8885011076927185,
      "learning_rate": 0.00017250000000000002,
      "loss": 2.1989,
      "step": 190
    },
    {
      "epoch": 0.9280742459396751,
      "grad_norm": 0.9783288836479187,
      "learning_rate": 0.00017093750000000003,
      "loss": 2.2916,
      "step": 200
    },
    {
      "epoch": 0.974477958236659,
      "grad_norm": 0.998541533946991,
      "learning_rate": 0.000169375,
      "loss": 2.198,
      "step": 210
    },
    {
      "epoch": 1.0185614849187936,
      "grad_norm": 1.025097131729126,
      "learning_rate": 0.00016781250000000002,
      "loss": 2.1896,
      "step": 220
    },
    {
      "epoch": 1.0649651972157772,
      "grad_norm": 0.9247034192085266,
      "learning_rate": 0.00016625000000000003,
      "loss": 2.1039,
      "step": 230
    },
    {
      "epoch": 1.111368909512761,
      "grad_norm": 1.0053119659423828,
      "learning_rate": 0.0001646875,
      "loss": 2.1522,
      "step": 240
    },
    {
      "epoch": 1.1577726218097448,
      "grad_norm": 1.0046968460083008,
      "learning_rate": 0.00016312500000000002,
      "loss": 2.02,
      "step": 250
    },
    {
      "epoch": 1.2041763341067284,
      "grad_norm": 0.9889487028121948,
      "learning_rate": 0.0001615625,
      "loss": 2.1105,
      "step": 260
    },
    {
      "epoch": 1.2505800464037122,
      "grad_norm": 1.0450915098190308,
      "learning_rate": 0.00016,
      "loss": 2.0902,
      "step": 270
    },
    {
      "epoch": 1.296983758700696,
      "grad_norm": 0.9516053199768066,
      "learning_rate": 0.00015843750000000002,
      "loss": 2.1114,
      "step": 280
    },
    {
      "epoch": 1.3433874709976799,
      "grad_norm": 0.9181150794029236,
      "learning_rate": 0.000156875,
      "loss": 2.1112,
      "step": 290
    },
    {
      "epoch": 1.3897911832946637,
      "grad_norm": 0.9718113541603088,
      "learning_rate": 0.00015531250000000001,
      "loss": 2.1155,
      "step": 300
    },
    {
      "epoch": 1.4361948955916473,
      "grad_norm": 0.9820765852928162,
      "learning_rate": 0.00015375000000000002,
      "loss": 2.0647,
      "step": 310
    },
    {
      "epoch": 1.482598607888631,
      "grad_norm": 0.9604142308235168,
      "learning_rate": 0.0001521875,
      "loss": 2.2404,
      "step": 320
    },
    {
      "epoch": 1.5290023201856149,
      "grad_norm": 1.0029507875442505,
      "learning_rate": 0.00015062500000000002,
      "loss": 2.129,
      "step": 330
    },
    {
      "epoch": 1.5754060324825985,
      "grad_norm": 1.029754638671875,
      "learning_rate": 0.00014906250000000003,
      "loss": 1.9318,
      "step": 340
    },
    {
      "epoch": 1.6218097447795823,
      "grad_norm": 1.0608818531036377,
      "learning_rate": 0.0001475,
      "loss": 2.147,
      "step": 350
    },
    {
      "epoch": 1.668213457076566,
      "grad_norm": 1.0544503927230835,
      "learning_rate": 0.00014593750000000002,
      "loss": 1.9644,
      "step": 360
    },
    {
      "epoch": 1.71461716937355,
      "grad_norm": 0.9582719802856445,
      "learning_rate": 0.00014437500000000003,
      "loss": 2.0778,
      "step": 370
    },
    {
      "epoch": 1.7610208816705337,
      "grad_norm": 1.3792132139205933,
      "learning_rate": 0.0001428125,
      "loss": 2.1737,
      "step": 380
    },
    {
      "epoch": 1.8074245939675175,
      "grad_norm": 1.0104174613952637,
      "learning_rate": 0.00014125000000000002,
      "loss": 1.9305,
      "step": 390
    },
    {
      "epoch": 1.8538283062645011,
      "grad_norm": 1.1613487005233765,
      "learning_rate": 0.00013968750000000003,
      "loss": 2.1138,
      "step": 400
    },
    {
      "epoch": 1.900232018561485,
      "grad_norm": 1.0683975219726562,
      "learning_rate": 0.000138125,
      "loss": 2.1907,
      "step": 410
    },
    {
      "epoch": 1.9466357308584685,
      "grad_norm": 1.1297764778137207,
      "learning_rate": 0.00013656250000000002,
      "loss": 2.1024,
      "step": 420
    },
    {
      "epoch": 1.9930394431554523,
      "grad_norm": 1.0339713096618652,
      "learning_rate": 0.00013500000000000003,
      "loss": 1.8814,
      "step": 430
    },
    {
      "epoch": 2.0371229698375872,
      "grad_norm": 1.1570630073547363,
      "learning_rate": 0.0001334375,
      "loss": 1.9716,
      "step": 440
    },
    {
      "epoch": 2.0835266821345706,
      "grad_norm": 1.0327129364013672,
      "learning_rate": 0.00013187500000000002,
      "loss": 2.0327,
      "step": 450
    },
    {
      "epoch": 2.1299303944315544,
      "grad_norm": 1.0023373365402222,
      "learning_rate": 0.0001303125,
      "loss": 2.0193,
      "step": 460
    },
    {
      "epoch": 2.176334106728538,
      "grad_norm": 1.1038061380386353,
      "learning_rate": 0.00012875,
      "loss": 2.0805,
      "step": 470
    },
    {
      "epoch": 2.222737819025522,
      "grad_norm": 1.2110785245895386,
      "learning_rate": 0.00012718750000000002,
      "loss": 2.1075,
      "step": 480
    },
    {
      "epoch": 2.269141531322506,
      "grad_norm": 1.1617549657821655,
      "learning_rate": 0.000125625,
      "loss": 2.0099,
      "step": 490
    },
    {
      "epoch": 2.3155452436194897,
      "grad_norm": 1.1749894618988037,
      "learning_rate": 0.00012406250000000001,
      "loss": 2.1016,
      "step": 500
    },
    {
      "epoch": 2.3619489559164735,
      "grad_norm": 1.0684106349945068,
      "learning_rate": 0.00012250000000000002,
      "loss": 2.0241,
      "step": 510
    },
    {
      "epoch": 2.408352668213457,
      "grad_norm": 1.0781461000442505,
      "learning_rate": 0.00012093750000000002,
      "loss": 2.0864,
      "step": 520
    },
    {
      "epoch": 2.4547563805104406,
      "grad_norm": 1.349022626876831,
      "learning_rate": 0.00011937500000000001,
      "loss": 2.1654,
      "step": 530
    },
    {
      "epoch": 2.5011600928074245,
      "grad_norm": 1.0301764011383057,
      "learning_rate": 0.00011781250000000001,
      "loss": 2.0199,
      "step": 540
    },
    {
      "epoch": 2.5475638051044083,
      "grad_norm": 1.1571253538131714,
      "learning_rate": 0.00011625000000000002,
      "loss": 1.9036,
      "step": 550
    },
    {
      "epoch": 2.593967517401392,
      "grad_norm": 1.0966713428497314,
      "learning_rate": 0.00011468750000000002,
      "loss": 1.8879,
      "step": 560
    },
    {
      "epoch": 2.640371229698376,
      "grad_norm": 1.077258586883545,
      "learning_rate": 0.00011312500000000001,
      "loss": 2.1191,
      "step": 570
    },
    {
      "epoch": 2.6867749419953597,
      "grad_norm": 1.2973823547363281,
      "learning_rate": 0.00011156250000000001,
      "loss": 1.9567,
      "step": 580
    },
    {
      "epoch": 2.7331786542923435,
      "grad_norm": 1.2633112668991089,
      "learning_rate": 0.00011000000000000002,
      "loss": 2.1407,
      "step": 590
    },
    {
      "epoch": 2.7795823665893273,
      "grad_norm": 1.2341645956039429,
      "learning_rate": 0.00010843750000000001,
      "loss": 1.9507,
      "step": 600
    },
    {
      "epoch": 2.825986078886311,
      "grad_norm": 1.219704508781433,
      "learning_rate": 0.00010687500000000001,
      "loss": 2.1017,
      "step": 610
    },
    {
      "epoch": 2.8723897911832945,
      "grad_norm": 1.089190125465393,
      "learning_rate": 0.00010531250000000002,
      "loss": 1.9259,
      "step": 620
    },
    {
      "epoch": 2.9187935034802783,
      "grad_norm": 1.2709465026855469,
      "learning_rate": 0.00010375000000000001,
      "loss": 1.8801,
      "step": 630
    },
    {
      "epoch": 2.965197215777262,
      "grad_norm": 1.296565055847168,
      "learning_rate": 0.00010218750000000001,
      "loss": 1.9796,
      "step": 640
    },
    {
      "epoch": 3.0092807424593966,
      "grad_norm": 1.177459716796875,
      "learning_rate": 0.00010062500000000002,
      "loss": 2.0174,
      "step": 650
    },
    {
      "epoch": 3.0556844547563804,
      "grad_norm": 1.4112945795059204,
      "learning_rate": 9.90625e-05,
      "loss": 2.0875,
      "step": 660
    },
    {
      "epoch": 3.102088167053364,
      "grad_norm": 8.028876304626465,
      "learning_rate": 9.75e-05,
      "loss": 1.9371,
      "step": 670
    },
    {
      "epoch": 3.148491879350348,
      "grad_norm": 1.1525523662567139,
      "learning_rate": 9.593750000000001e-05,
      "loss": 1.9327,
      "step": 680
    },
    {
      "epoch": 3.194895591647332,
      "grad_norm": 1.4623973369598389,
      "learning_rate": 9.4375e-05,
      "loss": 1.9896,
      "step": 690
    },
    {
      "epoch": 3.2412993039443156,
      "grad_norm": 1.3111727237701416,
      "learning_rate": 9.28125e-05,
      "loss": 1.9707,
      "step": 700
    },
    {
      "epoch": 3.2877030162412995,
      "grad_norm": 1.1998372077941895,
      "learning_rate": 9.125e-05,
      "loss": 1.8139,
      "step": 710
    },
    {
      "epoch": 3.3341067285382833,
      "grad_norm": 1.2301167249679565,
      "learning_rate": 8.96875e-05,
      "loss": 2.0132,
      "step": 720
    },
    {
      "epoch": 3.3805104408352666,
      "grad_norm": 1.2912907600402832,
      "learning_rate": 8.8125e-05,
      "loss": 1.9632,
      "step": 730
    },
    {
      "epoch": 3.4269141531322505,
      "grad_norm": 1.2738322019577026,
      "learning_rate": 8.65625e-05,
      "loss": 1.9756,
      "step": 740
    },
    {
      "epoch": 3.4733178654292343,
      "grad_norm": 1.308451533317566,
      "learning_rate": 8.5e-05,
      "loss": 1.9787,
      "step": 750
    },
    {
      "epoch": 3.519721577726218,
      "grad_norm": 1.1486471891403198,
      "learning_rate": 8.34375e-05,
      "loss": 2.044,
      "step": 760
    },
    {
      "epoch": 3.566125290023202,
      "grad_norm": 2.4516899585723877,
      "learning_rate": 8.1875e-05,
      "loss": 1.9413,
      "step": 770
    },
    {
      "epoch": 3.6125290023201857,
      "grad_norm": 1.2091941833496094,
      "learning_rate": 8.031250000000001e-05,
      "loss": 1.9874,
      "step": 780
    },
    {
      "epoch": 3.6589327146171695,
      "grad_norm": 1.3831608295440674,
      "learning_rate": 7.875e-05,
      "loss": 2.0703,
      "step": 790
    },
    {
      "epoch": 3.705336426914153,
      "grad_norm": 1.3119109869003296,
      "learning_rate": 7.71875e-05,
      "loss": 2.0718,
      "step": 800
    },
    {
      "epoch": 3.7517401392111367,
      "grad_norm": 2.1688144207000732,
      "learning_rate": 7.5625e-05,
      "loss": 2.0227,
      "step": 810
    },
    {
      "epoch": 3.7981438515081205,
      "grad_norm": 1.3914577960968018,
      "learning_rate": 7.40625e-05,
      "loss": 1.9572,
      "step": 820
    },
    {
      "epoch": 3.8445475638051043,
      "grad_norm": 1.3629498481750488,
      "learning_rate": 7.25e-05,
      "loss": 1.9913,
      "step": 830
    },
    {
      "epoch": 3.890951276102088,
      "grad_norm": 1.828856348991394,
      "learning_rate": 7.09375e-05,
      "loss": 1.8784,
      "step": 840
    },
    {
      "epoch": 3.937354988399072,
      "grad_norm": 2.1755847930908203,
      "learning_rate": 6.9375e-05,
      "loss": 1.9571,
      "step": 850
    },
    {
      "epoch": 3.9837587006960558,
      "grad_norm": 1.2117221355438232,
      "learning_rate": 6.78125e-05,
      "loss": 1.8874,
      "step": 860
    },
    {
      "epoch": 4.027842227378191,
      "grad_norm": 1.6364519596099854,
      "learning_rate": 6.625e-05,
      "loss": 2.0008,
      "step": 870
    },
    {
      "epoch": 4.0742459396751745,
      "grad_norm": 1.2211352586746216,
      "learning_rate": 6.468750000000001e-05,
      "loss": 1.9849,
      "step": 880
    },
    {
      "epoch": 4.120649651972157,
      "grad_norm": 1.4788912534713745,
      "learning_rate": 6.3125e-05,
      "loss": 1.9542,
      "step": 890
    },
    {
      "epoch": 4.167053364269141,
      "grad_norm": 1.0833779573440552,
      "learning_rate": 6.15625e-05,
      "loss": 1.8766,
      "step": 900
    },
    {
      "epoch": 4.213457076566125,
      "grad_norm": 1.1749467849731445,
      "learning_rate": 6e-05,
      "loss": 2.0058,
      "step": 910
    },
    {
      "epoch": 4.259860788863109,
      "grad_norm": 1.3381794691085815,
      "learning_rate": 5.84375e-05,
      "loss": 2.0306,
      "step": 920
    },
    {
      "epoch": 4.306264501160093,
      "grad_norm": 1.2382214069366455,
      "learning_rate": 5.6875e-05,
      "loss": 1.9129,
      "step": 930
    },
    {
      "epoch": 4.352668213457076,
      "grad_norm": 1.4930936098098755,
      "learning_rate": 5.53125e-05,
      "loss": 2.0345,
      "step": 940
    },
    {
      "epoch": 4.39907192575406,
      "grad_norm": 1.2630308866500854,
      "learning_rate": 5.375e-05,
      "loss": 2.0332,
      "step": 950
    },
    {
      "epoch": 4.445475638051044,
      "grad_norm": 1.246885895729065,
      "learning_rate": 5.21875e-05,
      "loss": 1.9082,
      "step": 960
    },
    {
      "epoch": 4.491879350348028,
      "grad_norm": 1.3524969816207886,
      "learning_rate": 5.0625e-05,
      "loss": 1.9725,
      "step": 970
    },
    {
      "epoch": 4.538283062645012,
      "grad_norm": 1.3164310455322266,
      "learning_rate": 4.90625e-05,
      "loss": 1.8235,
      "step": 980
    },
    {
      "epoch": 4.5846867749419955,
      "grad_norm": 1.9521894454956055,
      "learning_rate": 4.75e-05,
      "loss": 1.8937,
      "step": 990
    },
    {
      "epoch": 4.631090487238979,
      "grad_norm": 1.2728290557861328,
      "learning_rate": 4.59375e-05,
      "loss": 1.9879,
      "step": 1000
    },
    {
      "epoch": 4.677494199535963,
      "grad_norm": 1.2737740278244019,
      "learning_rate": 4.4375e-05,
      "loss": 2.0445,
      "step": 1010
    },
    {
      "epoch": 4.723897911832947,
      "grad_norm": 1.1709973812103271,
      "learning_rate": 4.28125e-05,
      "loss": 1.9642,
      "step": 1020
    },
    {
      "epoch": 4.770301624129931,
      "grad_norm": 1.3063772916793823,
      "learning_rate": 4.125e-05,
      "loss": 1.9675,
      "step": 1030
    },
    {
      "epoch": 4.816705336426914,
      "grad_norm": 1.3241121768951416,
      "learning_rate": 3.96875e-05,
      "loss": 1.8405,
      "step": 1040
    },
    {
      "epoch": 4.8631090487238975,
      "grad_norm": 1.4874897003173828,
      "learning_rate": 3.8125e-05,
      "loss": 1.8667,
      "step": 1050
    },
    {
      "epoch": 4.909512761020881,
      "grad_norm": 1.3769840002059937,
      "learning_rate": 3.65625e-05,
      "loss": 1.9399,
      "step": 1060
    },
    {
      "epoch": 4.955916473317865,
      "grad_norm": 1.3502343893051147,
      "learning_rate": 3.5e-05,
      "loss": 1.9621,
      "step": 1070
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.8678743839263916,
      "learning_rate": 3.34375e-05,
      "loss": 1.9267,
      "step": 1080
    }
  ],
  "logging_steps": 10,
  "max_steps": 1290,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.850047500386304e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
