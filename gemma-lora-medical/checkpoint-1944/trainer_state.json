{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 9.0,
  "eval_steps": 500,
  "global_step": 1944,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04640371229698376,
      "grad_norm": 29.752897262573242,
      "learning_rate": 0.00012,
      "loss": 3.4297,
      "step": 10
    },
    {
      "epoch": 0.09280742459396751,
      "grad_norm": 0.6792834401130676,
      "learning_rate": 0.0001994392523364486,
      "loss": 0.3672,
      "step": 20
    },
    {
      "epoch": 0.13921113689095127,
      "grad_norm": 0.3167804777622223,
      "learning_rate": 0.00019850467289719628,
      "loss": 0.1328,
      "step": 30
    },
    {
      "epoch": 0.18561484918793503,
      "grad_norm": 0.2067093402147293,
      "learning_rate": 0.00019757009345794393,
      "loss": 0.0723,
      "step": 40
    },
    {
      "epoch": 0.23201856148491878,
      "grad_norm": 0.24623557925224304,
      "learning_rate": 0.00019663551401869161,
      "loss": 0.0425,
      "step": 50
    },
    {
      "epoch": 0.27842227378190254,
      "grad_norm": 0.1861143559217453,
      "learning_rate": 0.00019570093457943924,
      "loss": 0.0248,
      "step": 60
    },
    {
      "epoch": 0.3248259860788863,
      "grad_norm": 0.14748965203762054,
      "learning_rate": 0.00019476635514018692,
      "loss": 0.009,
      "step": 70
    },
    {
      "epoch": 0.37122969837587005,
      "grad_norm": 0.1299159824848175,
      "learning_rate": 0.00019383177570093458,
      "loss": 0.0062,
      "step": 80
    },
    {
      "epoch": 0.4176334106728538,
      "grad_norm": 0.12962192296981812,
      "learning_rate": 0.00019289719626168226,
      "loss": 0.0043,
      "step": 90
    },
    {
      "epoch": 0.46403712296983757,
      "grad_norm": 0.042526908218860626,
      "learning_rate": 0.00019196261682242991,
      "loss": 0.0032,
      "step": 100
    },
    {
      "epoch": 0.5104408352668214,
      "grad_norm": 0.11202805489301682,
      "learning_rate": 0.0001910280373831776,
      "loss": 0.0036,
      "step": 110
    },
    {
      "epoch": 0.5568445475638051,
      "grad_norm": 0.08743245154619217,
      "learning_rate": 0.00019009345794392522,
      "loss": 0.0031,
      "step": 120
    },
    {
      "epoch": 0.6032482598607889,
      "grad_norm": 0.10391540825366974,
      "learning_rate": 0.0001891588785046729,
      "loss": 0.0034,
      "step": 130
    },
    {
      "epoch": 0.6496519721577726,
      "grad_norm": 0.1281067281961441,
      "learning_rate": 0.00018822429906542056,
      "loss": 0.0032,
      "step": 140
    },
    {
      "epoch": 0.6960556844547564,
      "grad_norm": 0.10307890176773071,
      "learning_rate": 0.00018728971962616824,
      "loss": 0.0034,
      "step": 150
    },
    {
      "epoch": 0.7424593967517401,
      "grad_norm": 0.03866773098707199,
      "learning_rate": 0.0001863551401869159,
      "loss": 0.0033,
      "step": 160
    },
    {
      "epoch": 0.7888631090487239,
      "grad_norm": 0.0366903692483902,
      "learning_rate": 0.00018542056074766355,
      "loss": 0.0037,
      "step": 170
    },
    {
      "epoch": 0.8352668213457076,
      "grad_norm": 0.05793609842658043,
      "learning_rate": 0.0001844859813084112,
      "loss": 0.0039,
      "step": 180
    },
    {
      "epoch": 0.8816705336426914,
      "grad_norm": 0.07077521830797195,
      "learning_rate": 0.00018355140186915889,
      "loss": 0.0032,
      "step": 190
    },
    {
      "epoch": 0.9280742459396751,
      "grad_norm": 0.04218196123838425,
      "learning_rate": 0.00018261682242990654,
      "loss": 0.0033,
      "step": 200
    },
    {
      "epoch": 0.974477958236659,
      "grad_norm": 0.017982812598347664,
      "learning_rate": 0.00018168224299065422,
      "loss": 0.0026,
      "step": 210
    },
    {
      "epoch": 1.0185614849187936,
      "grad_norm": 0.010860239155590534,
      "learning_rate": 0.00018074766355140188,
      "loss": 0.0027,
      "step": 220
    },
    {
      "epoch": 1.0649651972157772,
      "grad_norm": 0.05092265084385872,
      "learning_rate": 0.00017981308411214953,
      "loss": 0.0026,
      "step": 230
    },
    {
      "epoch": 1.111368909512761,
      "grad_norm": 0.06607115268707275,
      "learning_rate": 0.00017887850467289719,
      "loss": 0.0029,
      "step": 240
    },
    {
      "epoch": 1.1577726218097448,
      "grad_norm": 0.05460469052195549,
      "learning_rate": 0.00017794392523364487,
      "loss": 0.0024,
      "step": 250
    },
    {
      "epoch": 1.2041763341067284,
      "grad_norm": 0.05944018065929413,
      "learning_rate": 0.00017700934579439252,
      "loss": 0.0029,
      "step": 260
    },
    {
      "epoch": 1.2505800464037122,
      "grad_norm": 0.048181161284446716,
      "learning_rate": 0.0001760747663551402,
      "loss": 0.0029,
      "step": 270
    },
    {
      "epoch": 1.296983758700696,
      "grad_norm": 0.018248029053211212,
      "learning_rate": 0.00017514018691588786,
      "loss": 0.0024,
      "step": 280
    },
    {
      "epoch": 1.3433874709976799,
      "grad_norm": 0.0506916418671608,
      "learning_rate": 0.0001742056074766355,
      "loss": 0.0033,
      "step": 290
    },
    {
      "epoch": 1.3897911832946637,
      "grad_norm": 0.05208085849881172,
      "learning_rate": 0.00017327102803738317,
      "loss": 0.0033,
      "step": 300
    },
    {
      "epoch": 1.4361948955916473,
      "grad_norm": 0.047821369022130966,
      "learning_rate": 0.00017233644859813085,
      "loss": 0.0032,
      "step": 310
    },
    {
      "epoch": 1.482598607888631,
      "grad_norm": 0.053226809948682785,
      "learning_rate": 0.0001714018691588785,
      "loss": 0.0024,
      "step": 320
    },
    {
      "epoch": 1.5290023201856149,
      "grad_norm": 0.022054456174373627,
      "learning_rate": 0.00017046728971962618,
      "loss": 0.0032,
      "step": 330
    },
    {
      "epoch": 1.5754060324825985,
      "grad_norm": 0.05446939915418625,
      "learning_rate": 0.00016953271028037384,
      "loss": 0.0024,
      "step": 340
    },
    {
      "epoch": 1.6218097447795823,
      "grad_norm": 0.0711306780576706,
      "learning_rate": 0.0001685981308411215,
      "loss": 0.0026,
      "step": 350
    },
    {
      "epoch": 1.668213457076566,
      "grad_norm": 0.07831468433141708,
      "learning_rate": 0.00016766355140186915,
      "loss": 0.0027,
      "step": 360
    },
    {
      "epoch": 1.71461716937355,
      "grad_norm": 0.03774721175432205,
      "learning_rate": 0.00016672897196261683,
      "loss": 0.0031,
      "step": 370
    },
    {
      "epoch": 1.7610208816705337,
      "grad_norm": 0.03494058921933174,
      "learning_rate": 0.00016579439252336448,
      "loss": 0.0032,
      "step": 380
    },
    {
      "epoch": 1.8074245939675175,
      "grad_norm": 0.021825239062309265,
      "learning_rate": 0.00016485981308411217,
      "loss": 0.0031,
      "step": 390
    },
    {
      "epoch": 1.8538283062645011,
      "grad_norm": 0.05688163638114929,
      "learning_rate": 0.00016392523364485982,
      "loss": 0.0036,
      "step": 400
    },
    {
      "epoch": 1.900232018561485,
      "grad_norm": 0.061489783227443695,
      "learning_rate": 0.00016299065420560748,
      "loss": 0.0028,
      "step": 410
    },
    {
      "epoch": 1.9466357308584685,
      "grad_norm": 0.05117474123835564,
      "learning_rate": 0.00016205607476635513,
      "loss": 0.0032,
      "step": 420
    },
    {
      "epoch": 1.9930394431554523,
      "grad_norm": 0.044406354427337646,
      "learning_rate": 0.0001611214953271028,
      "loss": 0.0027,
      "step": 430
    },
    {
      "epoch": 2.0371229698375872,
      "grad_norm": 0.03531492128968239,
      "learning_rate": 0.00016018691588785047,
      "loss": 0.003,
      "step": 440
    },
    {
      "epoch": 2.0835266821345706,
      "grad_norm": 0.008366765454411507,
      "learning_rate": 0.00015925233644859815,
      "loss": 0.002,
      "step": 450
    },
    {
      "epoch": 2.1299303944315544,
      "grad_norm": 0.030965104699134827,
      "learning_rate": 0.0001583177570093458,
      "loss": 0.0027,
      "step": 460
    },
    {
      "epoch": 2.176334106728538,
      "grad_norm": 0.04249868914484978,
      "learning_rate": 0.00015738317757009346,
      "loss": 0.0022,
      "step": 470
    },
    {
      "epoch": 2.222737819025522,
      "grad_norm": 0.05971924960613251,
      "learning_rate": 0.00015644859813084114,
      "loss": 0.0029,
      "step": 480
    },
    {
      "epoch": 2.269141531322506,
      "grad_norm": 0.05411272123456001,
      "learning_rate": 0.0001555140186915888,
      "loss": 0.0028,
      "step": 490
    },
    {
      "epoch": 2.3155452436194897,
      "grad_norm": 0.06425105035305023,
      "learning_rate": 0.00015457943925233647,
      "loss": 0.0026,
      "step": 500
    },
    {
      "epoch": 2.3619489559164735,
      "grad_norm": 0.03324250876903534,
      "learning_rate": 0.00015364485981308413,
      "loss": 0.0024,
      "step": 510
    },
    {
      "epoch": 2.408352668213457,
      "grad_norm": 0.026802316308021545,
      "learning_rate": 0.00015271028037383178,
      "loss": 0.0024,
      "step": 520
    },
    {
      "epoch": 2.4547563805104406,
      "grad_norm": 0.06504109501838684,
      "learning_rate": 0.00015177570093457944,
      "loss": 0.0024,
      "step": 530
    },
    {
      "epoch": 2.5011600928074245,
      "grad_norm": 0.048093877732753754,
      "learning_rate": 0.00015084112149532712,
      "loss": 0.003,
      "step": 540
    },
    {
      "epoch": 2.5475638051044083,
      "grad_norm": 0.03047856129705906,
      "learning_rate": 0.00014990654205607477,
      "loss": 0.0022,
      "step": 550
    },
    {
      "epoch": 2.593967517401392,
      "grad_norm": 0.061947233974933624,
      "learning_rate": 0.00014897196261682246,
      "loss": 0.0028,
      "step": 560
    },
    {
      "epoch": 2.640371229698376,
      "grad_norm": 0.05565434321761131,
      "learning_rate": 0.0001480373831775701,
      "loss": 0.0028,
      "step": 570
    },
    {
      "epoch": 2.6867749419953597,
      "grad_norm": 0.010992348194122314,
      "learning_rate": 0.00014710280373831776,
      "loss": 0.0021,
      "step": 580
    },
    {
      "epoch": 2.7331786542923435,
      "grad_norm": 0.046822428703308105,
      "learning_rate": 0.00014616822429906542,
      "loss": 0.0028,
      "step": 590
    },
    {
      "epoch": 2.7795823665893273,
      "grad_norm": 0.01268205139786005,
      "learning_rate": 0.0001452336448598131,
      "loss": 0.0026,
      "step": 600
    },
    {
      "epoch": 2.825986078886311,
      "grad_norm": 0.03402166813611984,
      "learning_rate": 0.00014429906542056076,
      "loss": 0.0027,
      "step": 610
    },
    {
      "epoch": 2.8723897911832945,
      "grad_norm": 0.021858880296349525,
      "learning_rate": 0.00014336448598130844,
      "loss": 0.0027,
      "step": 620
    },
    {
      "epoch": 2.9187935034802783,
      "grad_norm": 0.030848544090986252,
      "learning_rate": 0.0001424299065420561,
      "loss": 0.0025,
      "step": 630
    },
    {
      "epoch": 2.965197215777262,
      "grad_norm": 0.04927149787545204,
      "learning_rate": 0.00014149532710280375,
      "loss": 0.0024,
      "step": 640
    },
    {
      "epoch": 3.0092807424593966,
      "grad_norm": 0.030709078535437584,
      "learning_rate": 0.0001405607476635514,
      "loss": 0.0024,
      "step": 650
    },
    {
      "epoch": 3.0556844547563804,
      "grad_norm": 0.06265505403280258,
      "learning_rate": 0.00013962616822429908,
      "loss": 0.0029,
      "step": 660
    },
    {
      "epoch": 3.102088167053364,
      "grad_norm": 0.05267404392361641,
      "learning_rate": 0.00013869158878504674,
      "loss": 0.0022,
      "step": 670
    },
    {
      "epoch": 3.148491879350348,
      "grad_norm": 0.05566844344139099,
      "learning_rate": 0.00013775700934579442,
      "loss": 0.0021,
      "step": 680
    },
    {
      "epoch": 3.194895591647332,
      "grad_norm": 0.03601917251944542,
      "learning_rate": 0.00013682242990654207,
      "loss": 0.0023,
      "step": 690
    },
    {
      "epoch": 3.2412993039443156,
      "grad_norm": 0.005659984890371561,
      "learning_rate": 0.00013588785046728973,
      "loss": 0.0023,
      "step": 700
    },
    {
      "epoch": 3.2877030162412995,
      "grad_norm": 0.04547784477472305,
      "learning_rate": 0.00013495327102803738,
      "loss": 0.0022,
      "step": 710
    },
    {
      "epoch": 3.3341067285382833,
      "grad_norm": 0.040029168128967285,
      "learning_rate": 0.00013401869158878506,
      "loss": 0.0027,
      "step": 720
    },
    {
      "epoch": 3.3805104408352666,
      "grad_norm": 0.05806870758533478,
      "learning_rate": 0.00013308411214953272,
      "loss": 0.0028,
      "step": 730
    },
    {
      "epoch": 3.4269141531322505,
      "grad_norm": 0.03537316620349884,
      "learning_rate": 0.0001321495327102804,
      "loss": 0.0024,
      "step": 740
    },
    {
      "epoch": 3.4733178654292343,
      "grad_norm": 0.02518133446574211,
      "learning_rate": 0.00013121495327102805,
      "loss": 0.0022,
      "step": 750
    },
    {
      "epoch": 3.519721577726218,
      "grad_norm": 0.052117347717285156,
      "learning_rate": 0.0001302803738317757,
      "loss": 0.0023,
      "step": 760
    },
    {
      "epoch": 3.566125290023202,
      "grad_norm": 0.02144543081521988,
      "learning_rate": 0.00012934579439252336,
      "loss": 0.0026,
      "step": 770
    },
    {
      "epoch": 3.6125290023201857,
      "grad_norm": 0.029463492333889008,
      "learning_rate": 0.00012841121495327104,
      "loss": 0.0026,
      "step": 780
    },
    {
      "epoch": 3.6589327146171695,
      "grad_norm": 0.04359698295593262,
      "learning_rate": 0.0001274766355140187,
      "loss": 0.0021,
      "step": 790
    },
    {
      "epoch": 3.705336426914153,
      "grad_norm": 0.039916105568408966,
      "learning_rate": 0.00012654205607476638,
      "loss": 0.0028,
      "step": 800
    },
    {
      "epoch": 3.7517401392111367,
      "grad_norm": 0.015398392453789711,
      "learning_rate": 0.000125607476635514,
      "loss": 0.0023,
      "step": 810
    },
    {
      "epoch": 3.7981438515081205,
      "grad_norm": 0.021159950643777847,
      "learning_rate": 0.0001246728971962617,
      "loss": 0.0024,
      "step": 820
    },
    {
      "epoch": 3.8445475638051043,
      "grad_norm": 0.027449985966086388,
      "learning_rate": 0.00012373831775700934,
      "loss": 0.0021,
      "step": 830
    },
    {
      "epoch": 3.890951276102088,
      "grad_norm": 0.11102575808763504,
      "learning_rate": 0.00012280373831775703,
      "loss": 0.0024,
      "step": 840
    },
    {
      "epoch": 3.937354988399072,
      "grad_norm": 0.03239814192056656,
      "learning_rate": 0.00012186915887850468,
      "loss": 0.0022,
      "step": 850
    },
    {
      "epoch": 3.9837587006960558,
      "grad_norm": 0.08645475655794144,
      "learning_rate": 0.00012093457943925235,
      "loss": 0.0026,
      "step": 860
    },
    {
      "epoch": 4.027842227378191,
      "grad_norm": 0.024245182052254677,
      "learning_rate": 0.00012,
      "loss": 0.0023,
      "step": 870
    },
    {
      "epoch": 4.0742459396751745,
      "grad_norm": 0.07642820477485657,
      "learning_rate": 0.00011906542056074767,
      "loss": 0.0022,
      "step": 880
    },
    {
      "epoch": 4.120649651972157,
      "grad_norm": 0.01684262417256832,
      "learning_rate": 0.00011813084112149533,
      "loss": 0.0021,
      "step": 890
    },
    {
      "epoch": 4.167053364269141,
      "grad_norm": 0.0745549276471138,
      "learning_rate": 0.00011719626168224301,
      "loss": 0.0024,
      "step": 900
    },
    {
      "epoch": 4.213457076566125,
      "grad_norm": 0.06984073668718338,
      "learning_rate": 0.00011626168224299065,
      "loss": 0.0023,
      "step": 910
    },
    {
      "epoch": 4.259860788863109,
      "grad_norm": 0.04264575615525246,
      "learning_rate": 0.00011532710280373833,
      "loss": 0.0023,
      "step": 920
    },
    {
      "epoch": 4.306264501160093,
      "grad_norm": 0.061048854142427444,
      "learning_rate": 0.00011439252336448598,
      "loss": 0.0028,
      "step": 930
    },
    {
      "epoch": 4.352668213457076,
      "grad_norm": 0.05891532823443413,
      "learning_rate": 0.00011345794392523365,
      "loss": 0.0019,
      "step": 940
    },
    {
      "epoch": 4.39907192575406,
      "grad_norm": 0.03689764067530632,
      "learning_rate": 0.00011252336448598131,
      "loss": 0.0025,
      "step": 950
    },
    {
      "epoch": 4.445475638051044,
      "grad_norm": 0.032327763736248016,
      "learning_rate": 0.00011158878504672899,
      "loss": 0.0023,
      "step": 960
    },
    {
      "epoch": 4.491879350348028,
      "grad_norm": 0.028931425884366035,
      "learning_rate": 0.00011065420560747663,
      "loss": 0.0022,
      "step": 970
    },
    {
      "epoch": 4.538283062645012,
      "grad_norm": 0.014807326719164848,
      "learning_rate": 0.00010971962616822431,
      "loss": 0.0019,
      "step": 980
    },
    {
      "epoch": 4.5846867749419955,
      "grad_norm": 0.05651453509926796,
      "learning_rate": 0.00010878504672897197,
      "loss": 0.0018,
      "step": 990
    },
    {
      "epoch": 4.631090487238979,
      "grad_norm": 0.03849111869931221,
      "learning_rate": 0.00010785046728971963,
      "loss": 0.0022,
      "step": 1000
    },
    {
      "epoch": 4.677494199535963,
      "grad_norm": 0.03816527873277664,
      "learning_rate": 0.00010691588785046729,
      "loss": 0.0022,
      "step": 1010
    },
    {
      "epoch": 4.723897911832947,
      "grad_norm": 0.05218551307916641,
      "learning_rate": 0.00010598130841121497,
      "loss": 0.0022,
      "step": 1020
    },
    {
      "epoch": 4.770301624129931,
      "grad_norm": 0.037785936146974564,
      "learning_rate": 0.00010504672897196261,
      "loss": 0.0021,
      "step": 1030
    },
    {
      "epoch": 4.816705336426914,
      "grad_norm": 0.03761189803481102,
      "learning_rate": 0.00010411214953271029,
      "loss": 0.0017,
      "step": 1040
    },
    {
      "epoch": 4.8631090487238975,
      "grad_norm": 0.04186568409204483,
      "learning_rate": 0.00010317757009345795,
      "loss": 0.0024,
      "step": 1050
    },
    {
      "epoch": 4.909512761020881,
      "grad_norm": 0.02978571504354477,
      "learning_rate": 0.00010224299065420561,
      "loss": 0.0022,
      "step": 1060
    },
    {
      "epoch": 4.955916473317865,
      "grad_norm": 0.052015986293554306,
      "learning_rate": 0.00010130841121495327,
      "loss": 0.0024,
      "step": 1070
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.11668407917022705,
      "learning_rate": 0.00010037383177570094,
      "loss": 0.0022,
      "step": 1080
    },
    {
      "epoch": 5.046403712296984,
      "grad_norm": 0.03562982380390167,
      "learning_rate": 9.94392523364486e-05,
      "loss": 0.0021,
      "step": 1090
    },
    {
      "epoch": 5.092807424593968,
      "grad_norm": 0.03243809938430786,
      "learning_rate": 9.850467289719627e-05,
      "loss": 0.0021,
      "step": 1100
    },
    {
      "epoch": 5.139211136890951,
      "grad_norm": 0.054953716695308685,
      "learning_rate": 9.757009345794393e-05,
      "loss": 0.0021,
      "step": 1110
    },
    {
      "epoch": 5.185614849187935,
      "grad_norm": 0.06359650939702988,
      "learning_rate": 9.66355140186916e-05,
      "loss": 0.0021,
      "step": 1120
    },
    {
      "epoch": 5.232018561484919,
      "grad_norm": 0.051464226096868515,
      "learning_rate": 9.570093457943926e-05,
      "loss": 0.0017,
      "step": 1130
    },
    {
      "epoch": 5.278422273781903,
      "grad_norm": 0.015606086701154709,
      "learning_rate": 9.476635514018692e-05,
      "loss": 0.0022,
      "step": 1140
    },
    {
      "epoch": 5.324825986078887,
      "grad_norm": 0.0632784441113472,
      "learning_rate": 9.383177570093459e-05,
      "loss": 0.0024,
      "step": 1150
    },
    {
      "epoch": 5.3712296983758705,
      "grad_norm": 0.04515873268246651,
      "learning_rate": 9.289719626168225e-05,
      "loss": 0.0021,
      "step": 1160
    },
    {
      "epoch": 5.417633410672853,
      "grad_norm": 0.028382357209920883,
      "learning_rate": 9.196261682242991e-05,
      "loss": 0.0022,
      "step": 1170
    },
    {
      "epoch": 5.464037122969837,
      "grad_norm": 0.03393203765153885,
      "learning_rate": 9.102803738317758e-05,
      "loss": 0.0019,
      "step": 1180
    },
    {
      "epoch": 5.510440835266821,
      "grad_norm": 0.041841402649879456,
      "learning_rate": 9.009345794392525e-05,
      "loss": 0.0019,
      "step": 1190
    },
    {
      "epoch": 5.556844547563805,
      "grad_norm": 0.03327095881104469,
      "learning_rate": 8.91588785046729e-05,
      "loss": 0.0018,
      "step": 1200
    },
    {
      "epoch": 5.603248259860789,
      "grad_norm": 0.03349605202674866,
      "learning_rate": 8.822429906542057e-05,
      "loss": 0.0021,
      "step": 1210
    },
    {
      "epoch": 5.6496519721577725,
      "grad_norm": 0.013739358633756638,
      "learning_rate": 8.728971962616824e-05,
      "loss": 0.0019,
      "step": 1220
    },
    {
      "epoch": 5.696055684454756,
      "grad_norm": 0.09354547411203384,
      "learning_rate": 8.635514018691589e-05,
      "loss": 0.0021,
      "step": 1230
    },
    {
      "epoch": 5.74245939675174,
      "grad_norm": 0.03836356848478317,
      "learning_rate": 8.542056074766356e-05,
      "loss": 0.002,
      "step": 1240
    },
    {
      "epoch": 5.788863109048724,
      "grad_norm": 0.025263626128435135,
      "learning_rate": 8.448598130841123e-05,
      "loss": 0.0018,
      "step": 1250
    },
    {
      "epoch": 5.835266821345708,
      "grad_norm": 0.013988444581627846,
      "learning_rate": 8.355140186915888e-05,
      "loss": 0.0016,
      "step": 1260
    },
    {
      "epoch": 5.8816705336426915,
      "grad_norm": 0.024346990510821342,
      "learning_rate": 8.261682242990655e-05,
      "loss": 0.002,
      "step": 1270
    },
    {
      "epoch": 5.928074245939675,
      "grad_norm": 0.04816018417477608,
      "learning_rate": 8.168224299065422e-05,
      "loss": 0.0017,
      "step": 1280
    },
    {
      "epoch": 5.974477958236659,
      "grad_norm": 0.0065653384663164616,
      "learning_rate": 8.074766355140187e-05,
      "loss": 0.002,
      "step": 1290
    },
    {
      "epoch": 6.018561484918793,
      "grad_norm": 0.04644916206598282,
      "learning_rate": 7.981308411214954e-05,
      "loss": 0.002,
      "step": 1300
    },
    {
      "epoch": 6.064965197215777,
      "grad_norm": 0.04819699749350548,
      "learning_rate": 7.887850467289721e-05,
      "loss": 0.0019,
      "step": 1310
    },
    {
      "epoch": 6.111368909512761,
      "grad_norm": 0.04147407412528992,
      "learning_rate": 7.794392523364486e-05,
      "loss": 0.002,
      "step": 1320
    },
    {
      "epoch": 6.157772621809745,
      "grad_norm": 0.045709893107414246,
      "learning_rate": 7.700934579439253e-05,
      "loss": 0.0023,
      "step": 1330
    },
    {
      "epoch": 6.204176334106728,
      "grad_norm": 0.032242387533187866,
      "learning_rate": 7.60747663551402e-05,
      "loss": 0.0018,
      "step": 1340
    },
    {
      "epoch": 6.250580046403712,
      "grad_norm": 0.0490972176194191,
      "learning_rate": 7.514018691588785e-05,
      "loss": 0.0014,
      "step": 1350
    },
    {
      "epoch": 6.296983758700696,
      "grad_norm": 0.025997035205364227,
      "learning_rate": 7.420560747663552e-05,
      "loss": 0.0015,
      "step": 1360
    },
    {
      "epoch": 6.34338747099768,
      "grad_norm": 0.002744843950495124,
      "learning_rate": 7.327102803738318e-05,
      "loss": 0.0017,
      "step": 1370
    },
    {
      "epoch": 6.389791183294664,
      "grad_norm": 0.03389642760157585,
      "learning_rate": 7.233644859813084e-05,
      "loss": 0.002,
      "step": 1380
    },
    {
      "epoch": 6.4361948955916475,
      "grad_norm": 0.01801445707678795,
      "learning_rate": 7.140186915887851e-05,
      "loss": 0.0017,
      "step": 1390
    },
    {
      "epoch": 6.482598607888631,
      "grad_norm": 0.030431324616074562,
      "learning_rate": 7.046728971962617e-05,
      "loss": 0.0015,
      "step": 1400
    },
    {
      "epoch": 6.529002320185615,
      "grad_norm": 0.04103569686412811,
      "learning_rate": 6.953271028037383e-05,
      "loss": 0.0017,
      "step": 1410
    },
    {
      "epoch": 6.575406032482599,
      "grad_norm": 0.015705877915024757,
      "learning_rate": 6.85981308411215e-05,
      "loss": 0.002,
      "step": 1420
    },
    {
      "epoch": 6.621809744779583,
      "grad_norm": 0.01608269475400448,
      "learning_rate": 6.766355140186916e-05,
      "loss": 0.0017,
      "step": 1430
    },
    {
      "epoch": 6.6682134570765665,
      "grad_norm": 0.023996734991669655,
      "learning_rate": 6.672897196261683e-05,
      "loss": 0.002,
      "step": 1440
    },
    {
      "epoch": 6.71461716937355,
      "grad_norm": 0.04183494672179222,
      "learning_rate": 6.57943925233645e-05,
      "loss": 0.0021,
      "step": 1450
    },
    {
      "epoch": 6.761020881670533,
      "grad_norm": 0.04547196626663208,
      "learning_rate": 6.485981308411215e-05,
      "loss": 0.0017,
      "step": 1460
    },
    {
      "epoch": 6.807424593967517,
      "grad_norm": 0.015935683622956276,
      "learning_rate": 6.392523364485982e-05,
      "loss": 0.002,
      "step": 1470
    },
    {
      "epoch": 6.853828306264501,
      "grad_norm": 0.03202391788363457,
      "learning_rate": 6.299065420560748e-05,
      "loss": 0.0017,
      "step": 1480
    },
    {
      "epoch": 6.900232018561485,
      "grad_norm": 0.04444102570414543,
      "learning_rate": 6.205607476635514e-05,
      "loss": 0.002,
      "step": 1490
    },
    {
      "epoch": 6.9466357308584685,
      "grad_norm": 0.02058870531618595,
      "learning_rate": 6.11214953271028e-05,
      "loss": 0.0016,
      "step": 1500
    },
    {
      "epoch": 6.993039443155452,
      "grad_norm": 0.060509078204631805,
      "learning_rate": 6.018691588785047e-05,
      "loss": 0.0018,
      "step": 1510
    },
    {
      "epoch": 7.037122969837587,
      "grad_norm": 0.008472995832562447,
      "learning_rate": 5.9252336448598136e-05,
      "loss": 0.0014,
      "step": 1520
    },
    {
      "epoch": 7.083526682134571,
      "grad_norm": 0.0564347580075264,
      "learning_rate": 5.83177570093458e-05,
      "loss": 0.0017,
      "step": 1530
    },
    {
      "epoch": 7.129930394431555,
      "grad_norm": 0.05405912548303604,
      "learning_rate": 5.738317757009346e-05,
      "loss": 0.0019,
      "step": 1540
    },
    {
      "epoch": 7.176334106728539,
      "grad_norm": 0.02477414160966873,
      "learning_rate": 5.644859813084113e-05,
      "loss": 0.0011,
      "step": 1550
    },
    {
      "epoch": 7.222737819025522,
      "grad_norm": 0.059445951133966446,
      "learning_rate": 5.551401869158879e-05,
      "loss": 0.0017,
      "step": 1560
    },
    {
      "epoch": 7.269141531322505,
      "grad_norm": 0.037436529994010925,
      "learning_rate": 5.457943925233645e-05,
      "loss": 0.0014,
      "step": 1570
    },
    {
      "epoch": 7.315545243619489,
      "grad_norm": 0.025012141093611717,
      "learning_rate": 5.364485981308412e-05,
      "loss": 0.0014,
      "step": 1580
    },
    {
      "epoch": 7.361948955916473,
      "grad_norm": 0.07325822114944458,
      "learning_rate": 5.271028037383178e-05,
      "loss": 0.0018,
      "step": 1590
    },
    {
      "epoch": 7.408352668213457,
      "grad_norm": 0.02046487107872963,
      "learning_rate": 5.177570093457944e-05,
      "loss": 0.0018,
      "step": 1600
    },
    {
      "epoch": 7.454756380510441,
      "grad_norm": 0.05872135981917381,
      "learning_rate": 5.08411214953271e-05,
      "loss": 0.0015,
      "step": 1610
    },
    {
      "epoch": 7.5011600928074245,
      "grad_norm": 0.021595468744635582,
      "learning_rate": 4.990654205607477e-05,
      "loss": 0.0015,
      "step": 1620
    },
    {
      "epoch": 7.547563805104408,
      "grad_norm": 0.004272268619388342,
      "learning_rate": 4.897196261682243e-05,
      "loss": 0.0016,
      "step": 1630
    },
    {
      "epoch": 7.593967517401392,
      "grad_norm": 0.05491018667817116,
      "learning_rate": 4.803738317757009e-05,
      "loss": 0.0015,
      "step": 1640
    },
    {
      "epoch": 7.640371229698376,
      "grad_norm": 0.021523889154195786,
      "learning_rate": 4.710280373831776e-05,
      "loss": 0.0015,
      "step": 1650
    },
    {
      "epoch": 7.68677494199536,
      "grad_norm": 0.052927128970623016,
      "learning_rate": 4.616822429906542e-05,
      "loss": 0.0017,
      "step": 1660
    },
    {
      "epoch": 7.7331786542923435,
      "grad_norm": 0.06777001172304153,
      "learning_rate": 4.523364485981308e-05,
      "loss": 0.0017,
      "step": 1670
    },
    {
      "epoch": 7.779582366589327,
      "grad_norm": 0.048160601407289505,
      "learning_rate": 4.429906542056075e-05,
      "loss": 0.0014,
      "step": 1680
    },
    {
      "epoch": 7.825986078886311,
      "grad_norm": 0.021220317110419273,
      "learning_rate": 4.336448598130841e-05,
      "loss": 0.0019,
      "step": 1690
    },
    {
      "epoch": 7.872389791183295,
      "grad_norm": 0.06072525307536125,
      "learning_rate": 4.242990654205607e-05,
      "loss": 0.0018,
      "step": 1700
    },
    {
      "epoch": 7.918793503480279,
      "grad_norm": 0.06520725786685944,
      "learning_rate": 4.149532710280374e-05,
      "loss": 0.0016,
      "step": 1710
    },
    {
      "epoch": 7.965197215777263,
      "grad_norm": 0.08299171924591064,
      "learning_rate": 4.05607476635514e-05,
      "loss": 0.002,
      "step": 1720
    },
    {
      "epoch": 8.009280742459397,
      "grad_norm": 0.04878094419836998,
      "learning_rate": 3.9626168224299064e-05,
      "loss": 0.0014,
      "step": 1730
    },
    {
      "epoch": 8.055684454756381,
      "grad_norm": 0.019046692177653313,
      "learning_rate": 3.869158878504673e-05,
      "loss": 0.0012,
      "step": 1740
    },
    {
      "epoch": 8.102088167053365,
      "grad_norm": 0.007255468517541885,
      "learning_rate": 3.775700934579439e-05,
      "loss": 0.0017,
      "step": 1750
    },
    {
      "epoch": 8.148491879350349,
      "grad_norm": 0.027726110070943832,
      "learning_rate": 3.6822429906542054e-05,
      "loss": 0.0016,
      "step": 1760
    },
    {
      "epoch": 8.194895591647331,
      "grad_norm": 0.05475277826189995,
      "learning_rate": 3.5887850467289716e-05,
      "loss": 0.0016,
      "step": 1770
    },
    {
      "epoch": 8.241299303944315,
      "grad_norm": 0.041418228298425674,
      "learning_rate": 3.495327102803739e-05,
      "loss": 0.0013,
      "step": 1780
    },
    {
      "epoch": 8.287703016241299,
      "grad_norm": 0.0036399387754499912,
      "learning_rate": 3.401869158878505e-05,
      "loss": 0.0014,
      "step": 1790
    },
    {
      "epoch": 8.334106728538282,
      "grad_norm": 0.02545126900076866,
      "learning_rate": 3.308411214953271e-05,
      "loss": 0.0016,
      "step": 1800
    },
    {
      "epoch": 8.380510440835266,
      "grad_norm": 0.010538558475673199,
      "learning_rate": 3.214953271028038e-05,
      "loss": 0.0015,
      "step": 1810
    },
    {
      "epoch": 8.42691415313225,
      "grad_norm": 0.01695704273879528,
      "learning_rate": 3.121495327102804e-05,
      "loss": 0.0016,
      "step": 1820
    },
    {
      "epoch": 8.473317865429234,
      "grad_norm": 0.047745607793331146,
      "learning_rate": 3.0280373831775704e-05,
      "loss": 0.0015,
      "step": 1830
    },
    {
      "epoch": 8.519721577726218,
      "grad_norm": 0.024403296411037445,
      "learning_rate": 2.934579439252337e-05,
      "loss": 0.002,
      "step": 1840
    },
    {
      "epoch": 8.566125290023201,
      "grad_norm": 0.030154377222061157,
      "learning_rate": 2.8411214953271033e-05,
      "loss": 0.0014,
      "step": 1850
    },
    {
      "epoch": 8.612529002320185,
      "grad_norm": 0.012281808070838451,
      "learning_rate": 2.7476635514018694e-05,
      "loss": 0.0017,
      "step": 1860
    },
    {
      "epoch": 8.658932714617169,
      "grad_norm": 0.06056945398449898,
      "learning_rate": 2.654205607476636e-05,
      "loss": 0.0016,
      "step": 1870
    },
    {
      "epoch": 8.705336426914153,
      "grad_norm": 0.06450524181127548,
      "learning_rate": 2.560747663551402e-05,
      "loss": 0.0018,
      "step": 1880
    },
    {
      "epoch": 8.751740139211137,
      "grad_norm": 0.1053711399435997,
      "learning_rate": 2.467289719626168e-05,
      "loss": 0.0018,
      "step": 1890
    },
    {
      "epoch": 8.79814385150812,
      "grad_norm": 0.02293618582189083,
      "learning_rate": 2.3738317757009346e-05,
      "loss": 0.0019,
      "step": 1900
    },
    {
      "epoch": 8.844547563805104,
      "grad_norm": 0.04403063654899597,
      "learning_rate": 2.2803738317757008e-05,
      "loss": 0.0015,
      "step": 1910
    },
    {
      "epoch": 8.890951276102088,
      "grad_norm": 0.046191297471523285,
      "learning_rate": 2.1869158878504676e-05,
      "loss": 0.0015,
      "step": 1920
    },
    {
      "epoch": 8.937354988399072,
      "grad_norm": 0.030034419149160385,
      "learning_rate": 2.093457943925234e-05,
      "loss": 0.0013,
      "step": 1930
    },
    {
      "epoch": 8.983758700696056,
      "grad_norm": 0.033764347434043884,
      "learning_rate": 2e-05,
      "loss": 0.0017,
      "step": 1940
    }
  ],
  "logging_steps": 10,
  "max_steps": 2150,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.330085500695347e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
