{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 1080,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04640371229698376,
      "grad_norm": 0.4521416425704956,
      "learning_rate": 0.00018,
      "loss": 2.3307,
      "step": 10
    },
    {
      "epoch": 0.09280742459396751,
      "grad_norm": 0.4067249894142151,
      "learning_rate": 0.00019859375,
      "loss": 2.0075,
      "step": 20
    },
    {
      "epoch": 0.13921113689095127,
      "grad_norm": 0.43062299489974976,
      "learning_rate": 0.00019703125,
      "loss": 1.8764,
      "step": 30
    },
    {
      "epoch": 0.18561484918793503,
      "grad_norm": 0.4481754004955292,
      "learning_rate": 0.00019546875,
      "loss": 1.8312,
      "step": 40
    },
    {
      "epoch": 0.23201856148491878,
      "grad_norm": 0.4786169230937958,
      "learning_rate": 0.00019390625000000001,
      "loss": 1.8959,
      "step": 50
    },
    {
      "epoch": 0.27842227378190254,
      "grad_norm": 0.5070240497589111,
      "learning_rate": 0.00019234375,
      "loss": 1.7901,
      "step": 60
    },
    {
      "epoch": 0.3248259860788863,
      "grad_norm": 0.6207846999168396,
      "learning_rate": 0.00019078125,
      "loss": 1.8318,
      "step": 70
    },
    {
      "epoch": 0.37122969837587005,
      "grad_norm": 0.5603432655334473,
      "learning_rate": 0.00018921875,
      "loss": 1.766,
      "step": 80
    },
    {
      "epoch": 0.4176334106728538,
      "grad_norm": 0.5932994484901428,
      "learning_rate": 0.00018765625,
      "loss": 1.6665,
      "step": 90
    },
    {
      "epoch": 0.46403712296983757,
      "grad_norm": 0.5440677404403687,
      "learning_rate": 0.00018609375,
      "loss": 1.7331,
      "step": 100
    },
    {
      "epoch": 0.5104408352668214,
      "grad_norm": 0.6049491167068481,
      "learning_rate": 0.00018453125,
      "loss": 1.7493,
      "step": 110
    },
    {
      "epoch": 0.5568445475638051,
      "grad_norm": 0.55955570936203,
      "learning_rate": 0.00018296875,
      "loss": 1.7053,
      "step": 120
    },
    {
      "epoch": 0.6032482598607889,
      "grad_norm": 0.6062839031219482,
      "learning_rate": 0.00018140625,
      "loss": 1.6166,
      "step": 130
    },
    {
      "epoch": 0.6496519721577726,
      "grad_norm": 0.6207166910171509,
      "learning_rate": 0.00017984375,
      "loss": 1.6767,
      "step": 140
    },
    {
      "epoch": 0.6960556844547564,
      "grad_norm": 0.596417248249054,
      "learning_rate": 0.00017828125,
      "loss": 1.5855,
      "step": 150
    },
    {
      "epoch": 0.7424593967517401,
      "grad_norm": 0.5228994488716125,
      "learning_rate": 0.00017671875,
      "loss": 1.6874,
      "step": 160
    },
    {
      "epoch": 0.7888631090487239,
      "grad_norm": 0.5279619097709656,
      "learning_rate": 0.00017515625,
      "loss": 1.5642,
      "step": 170
    },
    {
      "epoch": 0.8352668213457076,
      "grad_norm": 0.5999959111213684,
      "learning_rate": 0.00017359375,
      "loss": 1.5924,
      "step": 180
    },
    {
      "epoch": 0.8816705336426914,
      "grad_norm": 0.5472300052642822,
      "learning_rate": 0.00017203125,
      "loss": 1.6368,
      "step": 190
    },
    {
      "epoch": 0.9280742459396751,
      "grad_norm": 0.6315847039222717,
      "learning_rate": 0.00017046875,
      "loss": 1.7353,
      "step": 200
    },
    {
      "epoch": 0.974477958236659,
      "grad_norm": 0.6094098091125488,
      "learning_rate": 0.00016890625,
      "loss": 1.6419,
      "step": 210
    },
    {
      "epoch": 1.0185614849187936,
      "grad_norm": 0.5864531993865967,
      "learning_rate": 0.00016734375,
      "loss": 1.5518,
      "step": 220
    },
    {
      "epoch": 1.0649651972157772,
      "grad_norm": 0.6157925128936768,
      "learning_rate": 0.00016578125,
      "loss": 1.4822,
      "step": 230
    },
    {
      "epoch": 1.111368909512761,
      "grad_norm": 0.632568895816803,
      "learning_rate": 0.00016421875,
      "loss": 1.5171,
      "step": 240
    },
    {
      "epoch": 1.1577726218097448,
      "grad_norm": 0.6637520790100098,
      "learning_rate": 0.00016265624999999999,
      "loss": 1.4398,
      "step": 250
    },
    {
      "epoch": 1.2041763341067284,
      "grad_norm": 0.6639069318771362,
      "learning_rate": 0.00016109375,
      "loss": 1.4945,
      "step": 260
    },
    {
      "epoch": 1.2505800464037122,
      "grad_norm": 0.6508750319480896,
      "learning_rate": 0.00015953125,
      "loss": 1.4931,
      "step": 270
    },
    {
      "epoch": 1.296983758700696,
      "grad_norm": 0.6797022819519043,
      "learning_rate": 0.00015796875,
      "loss": 1.4988,
      "step": 280
    },
    {
      "epoch": 1.3433874709976799,
      "grad_norm": 0.6597998738288879,
      "learning_rate": 0.00015640625,
      "loss": 1.5142,
      "step": 290
    },
    {
      "epoch": 1.3897911832946637,
      "grad_norm": 0.6636275053024292,
      "learning_rate": 0.00015484375,
      "loss": 1.4929,
      "step": 300
    },
    {
      "epoch": 1.4361948955916473,
      "grad_norm": 0.7222744226455688,
      "learning_rate": 0.00015328125,
      "loss": 1.4369,
      "step": 310
    },
    {
      "epoch": 1.482598607888631,
      "grad_norm": 0.6860080361366272,
      "learning_rate": 0.00015171875,
      "loss": 1.5731,
      "step": 320
    },
    {
      "epoch": 1.5290023201856149,
      "grad_norm": 0.6608474850654602,
      "learning_rate": 0.00015015625,
      "loss": 1.5021,
      "step": 330
    },
    {
      "epoch": 1.5754060324825985,
      "grad_norm": 0.6267367005348206,
      "learning_rate": 0.00014859375,
      "loss": 1.3566,
      "step": 340
    },
    {
      "epoch": 1.6218097447795823,
      "grad_norm": 0.7142291069030762,
      "learning_rate": 0.00014703125,
      "loss": 1.5223,
      "step": 350
    },
    {
      "epoch": 1.668213457076566,
      "grad_norm": 0.6921085715293884,
      "learning_rate": 0.00014546875,
      "loss": 1.3938,
      "step": 360
    },
    {
      "epoch": 1.71461716937355,
      "grad_norm": 0.6568869352340698,
      "learning_rate": 0.00014390625,
      "loss": 1.4441,
      "step": 370
    },
    {
      "epoch": 1.7610208816705337,
      "grad_norm": 0.7842257618904114,
      "learning_rate": 0.00014234375,
      "loss": 1.4817,
      "step": 380
    },
    {
      "epoch": 1.8074245939675175,
      "grad_norm": 0.7196909189224243,
      "learning_rate": 0.00014078125,
      "loss": 1.3179,
      "step": 390
    },
    {
      "epoch": 1.8538283062645011,
      "grad_norm": 0.7390242218971252,
      "learning_rate": 0.00013921875,
      "loss": 1.4493,
      "step": 400
    },
    {
      "epoch": 1.900232018561485,
      "grad_norm": 0.7746609449386597,
      "learning_rate": 0.00013765625,
      "loss": 1.531,
      "step": 410
    },
    {
      "epoch": 1.9466357308584685,
      "grad_norm": 0.672615647315979,
      "learning_rate": 0.00013609374999999998,
      "loss": 1.4588,
      "step": 420
    },
    {
      "epoch": 1.9930394431554523,
      "grad_norm": 0.6969726085662842,
      "learning_rate": 0.00013453125,
      "loss": 1.3007,
      "step": 430
    },
    {
      "epoch": 2.0371229698375872,
      "grad_norm": 0.7486616969108582,
      "learning_rate": 0.00013296875,
      "loss": 1.2565,
      "step": 440
    },
    {
      "epoch": 2.0835266821345706,
      "grad_norm": 0.7506906986236572,
      "learning_rate": 0.00013140624999999999,
      "loss": 1.2877,
      "step": 450
    },
    {
      "epoch": 2.1299303944315544,
      "grad_norm": 0.7122743129730225,
      "learning_rate": 0.00012984375,
      "loss": 1.2868,
      "step": 460
    },
    {
      "epoch": 2.176334106728538,
      "grad_norm": 0.7743944525718689,
      "learning_rate": 0.00012828125,
      "loss": 1.3126,
      "step": 470
    },
    {
      "epoch": 2.222737819025522,
      "grad_norm": 0.9154590368270874,
      "learning_rate": 0.00012671875,
      "loss": 1.3153,
      "step": 480
    },
    {
      "epoch": 2.269141531322506,
      "grad_norm": 0.8956217169761658,
      "learning_rate": 0.00012515625,
      "loss": 1.2833,
      "step": 490
    },
    {
      "epoch": 2.3155452436194897,
      "grad_norm": 0.8947342038154602,
      "learning_rate": 0.00012359375,
      "loss": 1.3598,
      "step": 500
    },
    {
      "epoch": 2.3619489559164735,
      "grad_norm": 0.8213950395584106,
      "learning_rate": 0.00012203125,
      "loss": 1.3032,
      "step": 510
    },
    {
      "epoch": 2.408352668213457,
      "grad_norm": 0.752490758895874,
      "learning_rate": 0.00012046875,
      "loss": 1.2913,
      "step": 520
    },
    {
      "epoch": 2.4547563805104406,
      "grad_norm": 0.9207161068916321,
      "learning_rate": 0.00011890625,
      "loss": 1.2758,
      "step": 530
    },
    {
      "epoch": 2.5011600928074245,
      "grad_norm": 0.7816334962844849,
      "learning_rate": 0.00011734375,
      "loss": 1.2341,
      "step": 540
    },
    {
      "epoch": 2.5475638051044083,
      "grad_norm": 0.7639556527137756,
      "learning_rate": 0.00011578125,
      "loss": 1.2016,
      "step": 550
    },
    {
      "epoch": 2.593967517401392,
      "grad_norm": 0.8982826471328735,
      "learning_rate": 0.00011421875,
      "loss": 1.1219,
      "step": 560
    },
    {
      "epoch": 2.640371229698376,
      "grad_norm": 0.7494734525680542,
      "learning_rate": 0.00011265624999999999,
      "loss": 1.3108,
      "step": 570
    },
    {
      "epoch": 2.6867749419953597,
      "grad_norm": 0.8518854975700378,
      "learning_rate": 0.00011109375,
      "loss": 1.2584,
      "step": 580
    },
    {
      "epoch": 2.7331786542923435,
      "grad_norm": 0.8957260251045227,
      "learning_rate": 0.00010953125,
      "loss": 1.3545,
      "step": 590
    },
    {
      "epoch": 2.7795823665893273,
      "grad_norm": 0.8445538878440857,
      "learning_rate": 0.00010796874999999999,
      "loss": 1.2089,
      "step": 600
    },
    {
      "epoch": 2.825986078886311,
      "grad_norm": 0.8411957025527954,
      "learning_rate": 0.00010640625,
      "loss": 1.2841,
      "step": 610
    },
    {
      "epoch": 2.8723897911832945,
      "grad_norm": 0.7756429314613342,
      "learning_rate": 0.00010484375,
      "loss": 1.1659,
      "step": 620
    },
    {
      "epoch": 2.9187935034802783,
      "grad_norm": 1.0595623254776,
      "learning_rate": 0.00010328124999999999,
      "loss": 1.2079,
      "step": 630
    },
    {
      "epoch": 2.965197215777262,
      "grad_norm": 0.9708464741706848,
      "learning_rate": 0.00010171875,
      "loss": 1.2172,
      "step": 640
    },
    {
      "epoch": 3.0092807424593966,
      "grad_norm": 0.8351539373397827,
      "learning_rate": 0.00010015625,
      "loss": 1.1879,
      "step": 650
    },
    {
      "epoch": 3.0556844547563804,
      "grad_norm": 0.9170743823051453,
      "learning_rate": 9.859375000000001e-05,
      "loss": 1.1044,
      "step": 660
    },
    {
      "epoch": 3.102088167053364,
      "grad_norm": 1.0077214241027832,
      "learning_rate": 9.703125e-05,
      "loss": 1.0571,
      "step": 670
    },
    {
      "epoch": 3.148491879350348,
      "grad_norm": 0.9297031164169312,
      "learning_rate": 9.546875000000001e-05,
      "loss": 1.0453,
      "step": 680
    },
    {
      "epoch": 3.194895591647332,
      "grad_norm": 0.9880518913269043,
      "learning_rate": 9.390625000000001e-05,
      "loss": 1.079,
      "step": 690
    },
    {
      "epoch": 3.2412993039443156,
      "grad_norm": 1.0332649946212769,
      "learning_rate": 9.234375e-05,
      "loss": 1.0817,
      "step": 700
    },
    {
      "epoch": 3.2877030162412995,
      "grad_norm": 1.0722441673278809,
      "learning_rate": 9.078125e-05,
      "loss": 0.9639,
      "step": 710
    },
    {
      "epoch": 3.3341067285382833,
      "grad_norm": 1.1060717105865479,
      "learning_rate": 8.921875000000001e-05,
      "loss": 1.0885,
      "step": 720
    },
    {
      "epoch": 3.3805104408352666,
      "grad_norm": 1.217166543006897,
      "learning_rate": 8.765625e-05,
      "loss": 1.1018,
      "step": 730
    },
    {
      "epoch": 3.4269141531322505,
      "grad_norm": 1.0650835037231445,
      "learning_rate": 8.609375e-05,
      "loss": 1.1087,
      "step": 740
    },
    {
      "epoch": 3.4733178654292343,
      "grad_norm": 0.9869964718818665,
      "learning_rate": 8.453125000000001e-05,
      "loss": 1.0249,
      "step": 750
    },
    {
      "epoch": 3.519721577726218,
      "grad_norm": 0.9467852711677551,
      "learning_rate": 8.296875000000001e-05,
      "loss": 1.1378,
      "step": 760
    },
    {
      "epoch": 3.566125290023202,
      "grad_norm": 1.1236368417739868,
      "learning_rate": 8.140625e-05,
      "loss": 1.0106,
      "step": 770
    },
    {
      "epoch": 3.6125290023201857,
      "grad_norm": 1.316352367401123,
      "learning_rate": 7.984375e-05,
      "loss": 1.0339,
      "step": 780
    },
    {
      "epoch": 3.6589327146171695,
      "grad_norm": 1.127000331878662,
      "learning_rate": 7.828125000000001e-05,
      "loss": 1.1235,
      "step": 790
    },
    {
      "epoch": 3.705336426914153,
      "grad_norm": 1.0420277118682861,
      "learning_rate": 7.671875e-05,
      "loss": 1.1106,
      "step": 800
    },
    {
      "epoch": 3.7517401392111367,
      "grad_norm": 1.2421753406524658,
      "learning_rate": 7.515625e-05,
      "loss": 1.1475,
      "step": 810
    },
    {
      "epoch": 3.7981438515081205,
      "grad_norm": 1.054057002067566,
      "learning_rate": 7.359375000000001e-05,
      "loss": 1.079,
      "step": 820
    },
    {
      "epoch": 3.8445475638051043,
      "grad_norm": 1.1654044389724731,
      "learning_rate": 7.203125e-05,
      "loss": 1.1155,
      "step": 830
    },
    {
      "epoch": 3.890951276102088,
      "grad_norm": 0.7934538125991821,
      "learning_rate": 7.046875e-05,
      "loss": 1.0019,
      "step": 840
    },
    {
      "epoch": 3.937354988399072,
      "grad_norm": 1.1868804693222046,
      "learning_rate": 6.890625000000001e-05,
      "loss": 1.0075,
      "step": 850
    },
    {
      "epoch": 3.9837587006960558,
      "grad_norm": 1.034835696220398,
      "learning_rate": 6.734375000000001e-05,
      "loss": 0.9841,
      "step": 860
    },
    {
      "epoch": 4.027842227378191,
      "grad_norm": 1.0359834432601929,
      "learning_rate": 6.578125e-05,
      "loss": 1.0288,
      "step": 870
    },
    {
      "epoch": 4.0742459396751745,
      "grad_norm": 1.2473807334899902,
      "learning_rate": 6.421875e-05,
      "loss": 0.9499,
      "step": 880
    },
    {
      "epoch": 4.120649651972157,
      "grad_norm": 1.190578579902649,
      "learning_rate": 6.265625000000001e-05,
      "loss": 0.9333,
      "step": 890
    },
    {
      "epoch": 4.167053364269141,
      "grad_norm": 1.0425727367401123,
      "learning_rate": 6.109375e-05,
      "loss": 0.8904,
      "step": 900
    },
    {
      "epoch": 4.213457076566125,
      "grad_norm": 1.1839749813079834,
      "learning_rate": 5.953125000000001e-05,
      "loss": 0.8922,
      "step": 910
    },
    {
      "epoch": 4.259860788863109,
      "grad_norm": 1.135361909866333,
      "learning_rate": 5.796875e-05,
      "loss": 1.0149,
      "step": 920
    },
    {
      "epoch": 4.306264501160093,
      "grad_norm": 1.0800732374191284,
      "learning_rate": 5.6406250000000006e-05,
      "loss": 0.8606,
      "step": 930
    },
    {
      "epoch": 4.352668213457076,
      "grad_norm": 1.3091405630111694,
      "learning_rate": 5.484375e-05,
      "loss": 0.9039,
      "step": 940
    },
    {
      "epoch": 4.39907192575406,
      "grad_norm": 1.3173495531082153,
      "learning_rate": 5.3281250000000004e-05,
      "loss": 0.96,
      "step": 950
    },
    {
      "epoch": 4.445475638051044,
      "grad_norm": 1.1842514276504517,
      "learning_rate": 5.171875000000001e-05,
      "loss": 0.8999,
      "step": 960
    },
    {
      "epoch": 4.491879350348028,
      "grad_norm": 1.1499394178390503,
      "learning_rate": 5.015625e-05,
      "loss": 0.9342,
      "step": 970
    },
    {
      "epoch": 4.538283062645012,
      "grad_norm": 1.1635725498199463,
      "learning_rate": 4.8593750000000005e-05,
      "loss": 0.8704,
      "step": 980
    },
    {
      "epoch": 4.5846867749419955,
      "grad_norm": 1.3053228855133057,
      "learning_rate": 4.703125e-05,
      "loss": 0.9468,
      "step": 990
    },
    {
      "epoch": 4.631090487238979,
      "grad_norm": 1.386744499206543,
      "learning_rate": 4.5468750000000004e-05,
      "loss": 0.8379,
      "step": 1000
    },
    {
      "epoch": 4.677494199535963,
      "grad_norm": 1.1157004833221436,
      "learning_rate": 4.390625000000001e-05,
      "loss": 1.0278,
      "step": 1010
    },
    {
      "epoch": 4.723897911832947,
      "grad_norm": 1.1333001852035522,
      "learning_rate": 4.234375e-05,
      "loss": 0.8357,
      "step": 1020
    },
    {
      "epoch": 4.770301624129931,
      "grad_norm": 1.3118016719818115,
      "learning_rate": 4.0781250000000005e-05,
      "loss": 0.9249,
      "step": 1030
    },
    {
      "epoch": 4.816705336426914,
      "grad_norm": 1.3227946758270264,
      "learning_rate": 3.921875e-05,
      "loss": 0.8362,
      "step": 1040
    },
    {
      "epoch": 4.8631090487238975,
      "grad_norm": 1.1272368431091309,
      "learning_rate": 3.7656250000000004e-05,
      "loss": 0.7927,
      "step": 1050
    },
    {
      "epoch": 4.909512761020881,
      "grad_norm": 1.0585943460464478,
      "learning_rate": 3.6093750000000007e-05,
      "loss": 0.9316,
      "step": 1060
    },
    {
      "epoch": 4.955916473317865,
      "grad_norm": 1.2985631227493286,
      "learning_rate": 3.453125e-05,
      "loss": 0.9058,
      "step": 1070
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.5664515495300293,
      "learning_rate": 3.2968750000000005e-05,
      "loss": 0.833,
      "step": 1080
    }
  ],
  "logging_steps": 10,
  "max_steps": 1290,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.606802671763456e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
