{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cc6a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "from peft import LoraConfig\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db246fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------------------------------------------------\n",
    "train_df = pd.read_csv(\"medical_cases_train/medical_cases_train.csv\")\n",
    "val_df = pd.read_csv(\"medical_cases_validation/medical_cases_validation.csv\")\n",
    "test_df = pd.read_csv(\"medical_cases_test/medical_cases_test.csv\")\n",
    "\n",
    "train_set = Dataset.from_pandas(train_df)\n",
    "val_set = Dataset.from_pandas(val_df)\n",
    "test_set = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09cfb61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcefb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5925e8a7304e7b92fb5f861c28c2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5f363c38014ed69fada9475c6cc758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1351787b0a34dd6914248bdafcb319b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "c:\\Users\\abhay\\anaconda3\\envs\\assignment1\\Lib\\site-packages\\accelerate\\utils\\modeling.py:807: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.tensor([0], device=i)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f730f3301af4bf3a5eb2dc4a6f84988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe8e83c0ed6498b91e5e8dcbd744290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22dbb2a62c934c288271cd2b1e5f229e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhay\\AppData\\Local\\Temp\\ipykernel_25480\\1809444756.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\abhay\\anaconda3\\envs\\assignment1\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/215 00:28 < 1:39:57, 0.04 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"<start_of_turn>user\\nDescription:{example['description']}<end_of_turn> \\\n",
    "        \\n<start_of_turn>model\\n{example['medical_specialty']}<end_of_turn>\"\n",
    "    }\n",
    "\n",
    "train_dataset = train_set.map(format_prompt)\n",
    "val_dataset = val_set.map(format_prompt)\n",
    "test_dataset = test_set.map(format_prompt)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. LOAD MODEL & TOKENIZER (from Hugging Face Transformers)\n",
    "# -------------------------------------------------------------------\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False  # <-- This helps avoid TypeError in some cases\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# -------------------------------------------------------------------\n",
    "# 4. APPLY LoRA using PEFT\n",
    "# -------------------------------------------------------------------\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. TOKENIZATION\n",
    "# -------------------------------------------------------------------\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(tokenize, remove_columns=val_dataset.column_names)\n",
    "test_dataset = test_dataset.map(tokenize, remove_columns=test_dataset.column_names)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. TRAINING ARGUMENTS\n",
    "# -------------------------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepseek-lora-medical\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7. TRAINER\n",
    "# -------------------------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# -------------------------------------------------------------------\n",
    "# 9. SAVE MODEL\n",
    "# -------------------------------------------------------------------\n",
    "model.save_pretrained(\"./deepseek-lora-medical\")\n",
    "tokenizer.save_pretrained(\"./deepseek-lora-medical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f123d60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m target_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedical_specialty\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m      5\u001b[0m target_classes_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(target_classes)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      9\u001b[0m y_pt \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m y_gt \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------------------------\n",
    "# SETUP\n",
    "# -------------------------------------------------------------------\n",
    "target_classes = sorted(np.unique(test_df[\"medical_specialty\"]))\n",
    "target_classes_str = \"\\n\".join(target_classes)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "y_pt = []\n",
    "y_gt = []\n",
    "\n",
    "# Clear logs\n",
    "open(\"deepseek.txt\", \"w\").close()\n",
    "open(\"deepseek_unknown.txt\", \"w\").close()\n",
    "\n",
    "print(\"\\n=== Predictions on Test Set ===\\n\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# MATCHING FUNCTION\n",
    "# -------------------------------------------------------------------\n",
    "def match_class(prediction_raw, target_classes):\n",
    "    pred = prediction_raw.lower().strip()\n",
    "\n",
    "    # Exact match\n",
    "    for cls in target_classes:\n",
    "        if pred == cls.lower():\n",
    "            return cls\n",
    "\n",
    "    # Substring match\n",
    "    for cls in target_classes:\n",
    "        if cls.lower() in pred:\n",
    "            return cls\n",
    "\n",
    "    # Word overlap\n",
    "    pred_words = set(pred.split())\n",
    "    for cls in target_classes:\n",
    "        cls_words = set(cls.lower().split())\n",
    "        if pred_words & cls_words:\n",
    "            return cls\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# INFERENCE LOOP\n",
    "# -------------------------------------------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    true_label = test_df.iloc[i][\"medical_specialty\"]\n",
    "    description = test_df.iloc[i][\"description\"]\n",
    "\n",
    "    prompt = f\"\"\"Classify the following medical case description into one of the following medical specialties.\n",
    "\n",
    "Respond with only the name of the specialty. One-word answer. No explanations.\n",
    "\n",
    "Choices:\n",
    "{target_classes_str}\n",
    "\n",
    "Description:\n",
    "{description}\n",
    "\n",
    "Medical Specialty:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    prediction_raw = decoded.split(\"Medical Specialty:\")[-1].strip()\n",
    "\n",
    "    matched_class = match_class(prediction_raw, target_classes)\n",
    "\n",
    "    if matched_class == \"Unknown\":\n",
    "        with open(\"deepseek_unknown.txt\", \"a\") as f:\n",
    "            f.write(f\"[Unknown] Raw prediction: {prediction_raw}\\nDescription: {description}\\n\\n\")\n",
    "\n",
    "    y_pt.append(matched_class)\n",
    "    y_gt.append(true_label)\n",
    "\n",
    "    with open(\"deepseek.txt\", \"a\") as f:\n",
    "        f.write(f\"Prediction: {matched_class}\\n\")\n",
    "        f.write(f\"True Label: {true_label}\\n\\n\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# EVALUATION\n",
    "# -------------------------------------------------------------------\n",
    "filtered_preds = [p for p in y_pt if p != \"Unknown\"]\n",
    "filtered_truth = [t for p, t in zip(y_pt, y_gt) if p != \"Unknown\"]\n",
    "\n",
    "print(\"\\n=== Evaluation Metrics (Excluding 'Unknown') ===\")\n",
    "print(f\"Total predictions: {len(y_pt)}\")\n",
    "print(f\"Unknown predictions: {y_pt.count('Unknown')}\")\n",
    "print(\"Accuracy:\", accuracy_score(filtered_truth, filtered_preds))\n",
    "print(\"Precision:\", precision_score(filtered_truth, filtered_preds, average='macro', zero_division=0))\n",
    "print(\"Recall:\", recall_score(filtered_truth, filtered_preds, average='macro', zero_division=0))\n",
    "print(\"F1 Score:\", f1_score(filtered_truth, filtered_preds, average='macro', zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
