{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"hpe-ai/medical-cases-classification-tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37a1ea67554462f943cebc08dc1f5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb397e6fa5ea48c780fe4521ff24c693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1934edbc1a4e4a5b892840b640de8ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for split in ds:\n",
    "    ds[split].save_to_disk(f\"./medical_cases_{split}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9420116eec54456c87a017de918de2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b311787a2ec47c1965ed95fdaf1f4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  78%|#######7  | 1.55G/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e6f10fa25b4f78bce3d83e1aa54fab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebc56935c5d49d5ae17cddf7ac7e565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhay\\AppData\\Local\\Temp\\ipykernel_20236\\2597363362.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabhayvg-2904\u001b[0m (\u001b[33mabhayvg-2904-indian-institute-of-technology-gandhinagar\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Users\\abhay\\Downloads\\AISG\\Assignments\\AISG_Assignment_2\\wandb\\run-20250409_010200-s0v96l1s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abhayvg-2904-indian-institute-of-technology-gandhinagar/huggingface/runs/s0v96l1s' target=\"_blank\">./gemma-lora-medical</a></strong> to <a href='https://wandb.ai/abhayvg-2904-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abhayvg-2904-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">https://wandb.ai/abhayvg-2904-indian-institute-of-technology-gandhinagar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abhayvg-2904-indian-institute-of-technology-gandhinagar/huggingface/runs/s0v96l1s' target=\"_blank\">https://wandb.ai/abhayvg-2904-indian-institute-of-technology-gandhinagar/huggingface/runs/s0v96l1s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to train Gemma3 models with the `eager` attention implementation instead of `sdpa`. Use `eager` with `AutoModelForCausalLM.from_pretrained('<path-to-checkpoint>', attn_implementation='eager')`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='1293' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  71/1293 09:34 < 2:49:38, 0.12 it/s, Epoch 0.16/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. LOAD & PREPROCESS THE DATA\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Load the CSV dataset\n",
    "df = pd.read_csv(r\"medical_cases_train\\medical_cases_train.csv\")\n",
    "\n",
    "# Keep only the relevant columns and drop rows with missing values\n",
    "df = df[[\"description\", \"transcription\"]].dropna()\n",
    "\n",
    "# Convert the Pandas dataframe into a Hugging Face Dataset object\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Format each row as a prompt-response dialogue (chat style for Gemma)\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"<start_of_turn>user\\n{example['description']}\\n<end_of_turn>\\n<start_of_turn>model\\n{example['transcription']}<end_of_turn>\"\n",
    "    }\n",
    "\n",
    "# Apply the prompt formatting to each row in the dataset\n",
    "dataset = dataset.map(format_prompt)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. LOAD TOKENIZER AND BASE MODEL\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Set the name of the model checkpoint\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure that padding token is set (Gemma uses EOS token for padding too)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model in float16 precision and place it on available GPU automatically\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Use 16-bit precision (efficient on modern GPUs)\n",
    "    device_map=\"auto\"  # Automatically spreads model across available GPUs\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. APPLY LoRA FOR PARAMETER-EFFICIENT FINE-TUNING\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Define the LoRA configuration (Low-Rank Adaptation)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the update matrices (lower = less compute, higher = more expressive)\n",
    "    lora_alpha=32,  # Scaling factor for LoRA\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # These are the attention projection layers to inject LoRA into\n",
    "    lora_dropout=0.05,  # Dropout for regularization\n",
    "    bias=\"none\",  # Do not adapt the biases\n",
    "    task_type=TaskType.CAUSAL_LM  # Task is causal language modeling (predict next token)\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. TOKENIZE THE DATA\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Tokenization function that also prepares labels (for training)\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512  # You can adjust this based on GPU memory\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # Set labels for language modeling\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. SETUP TRAINING CONFIGURATION\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-lora-medical\",  # Directory to save model checkpoints\n",
    "    per_device_train_batch_size=1,  # Use batch size 1 (can be increased if GPU allows)\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch size\n",
    "    num_train_epochs=3,  # Train for 3 epochs\n",
    "    learning_rate=2e-4,  # Learning rate (tuned for LoRA)\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    logging_dir=\"./logs\",  # Where to write logs\n",
    "    save_strategy=\"epoch\",  # Save model every epoch\n",
    "    save_total_limit=2  # Keep only the latest 2 checkpoints\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. INITIALIZE THE TRAINER\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,  # LoRA-wrapped model\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,  # Tokenized dataset\n",
    "    tokenizer=tokenizer,  # Needed for padding and decoding\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)  # MLM=False for causal LM\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7. START TRAINING\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 8. SAVE THE FINE-TUNED MODEL AND TOKENIZER\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "model.save_pretrained(\"./gemma-lora-medical\")\n",
    "tokenizer.save_pretrained(\"./gemma-lora-medical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09db931d92b4d77ab10755642fa83fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8def955b6f4655aced68e4710567b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f322cb445c54ade9e4a387769e9f638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhay\\anaconda3\\envs\\assignment1\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3050 Ti Laptop GPU. Num GPUs = 1. Max memory: 4.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ed37ddfd024270a2a2ab6e7809e093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288b04aa4f1a46069721daf213c20bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069178d538a54af88c92eb3872e9e491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhay\\AppData\\Local\\Temp\\ipykernel_9432\\3614802169.py:116: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,724 | Num Epochs = 6 | Total steps = 1,290\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 745,472/1,000,000,000 (0.07% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='1290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 336/1290 21:23 < 1:01:05, 0.26 it/s, Epoch 1.55/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.959900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.610200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.504900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.387100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.339800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.322200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.079700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.198000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.110500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.090200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.240400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.129000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
    "import torch\n",
    "from peft import LoraConfig\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. LOAD DATA\n",
    "# -------------------------------------------------------------------\n",
    "train_df = pd.read_csv(\"medical_cases_train/medical_cases_train.csv\")[[\"description\", \"transcription\"]].dropna()\n",
    "val_df = pd.read_csv(\"medical_cases_validation/medical_cases_validation.csv\")[[\"description\", \"transcription\"]].dropna()\n",
    "test_df = pd.read_csv(\"medical_cases_test/medical_cases_test.csv\")[[\"description\", \"transcription\"]].dropna()\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. FORMAT PROMPTS\n",
    "# -------------------------------------------------------------------\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"<start_of_turn>user\\n{example['description']}\\n<end_of_turn>\\n<start_of_turn>model\\n{example['transcription']}<end_of_turn>\"\n",
    "    }\n",
    "\n",
    "train_dataset = train_dataset.map(format_prompt)\n",
    "val_dataset = val_dataset.map(format_prompt)\n",
    "test_dataset = test_dataset.map(format_prompt)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. LOAD MODEL\n",
    "# -------------------------------------------------------------------\n",
    "model_name = \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=512,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. APPLY LoRA\n",
    "# -------------------------------------------------------------------\n",
    "FastLanguageModel.for_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model.add_adapter(lora_config)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. TOKENIZATION\n",
    "# -------------------------------------------------------------------\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(tokenize, remove_columns=val_dataset.column_names)\n",
    "test_dataset = test_dataset.map(tokenize, remove_columns=test_dataset.column_names)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6. TRAINING ARGUMENTS\n",
    "# -------------------------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-lora-medical\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,\n",
    "    num_train_epochs=6,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7. METRICS FUNCTION\n",
    "# -------------------------------------------------------------------\n",
    "def compute_metrics(eval_pred: EvalPrediction):\n",
    "    preds = eval_pred.predictions.argmax(-1)\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    # Flatten and ignore padded tokens\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for pred, label in zip(preds, labels):\n",
    "        for p, l in zip(pred, label):\n",
    "            if l != -100:\n",
    "                true_labels.append(l)\n",
    "                pred_labels.append(p)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(true_labels, pred_labels),\n",
    "        \"precision\": precision_score(true_labels, pred_labels, average='macro', zero_division=0),\n",
    "        \"recall\": recall_score(true_labels, pred_labels, average='macro', zero_division=0),\n",
    "        \"f1\": f1_score(true_labels, pred_labels, average='macro', zero_division=0),\n",
    "    }\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 8. TRAINER\n",
    "# -------------------------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 9. FINAL TEST EVALUATION\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n=== Final Evaluation on Test Set ===\")\n",
    "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "for key, value in test_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 10. SAVE MODEL\n",
    "# -------------------------------------------------------------------\n",
    "model.save_pretrained(\"./gemma-lora-medical\")\n",
    "tokenizer.save_pretrained(\"./gemma-lora-medical\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhay\\AppData\\Local\\Temp\\ipykernel_12496\\1019256308.py:8: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhay\\anaconda3\\envs\\assignment1\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3050 Ti Laptop GPU. Num GPUs = 1. Max memory: 4.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "=== Predictions on Test Set ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/370 [00:08<52:28,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Laminectomy & Facetectomy\n",
      "Prediction: ENT - Otolaryngology\n",
      "True Label: Orthopedic\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/370 [00:11<31:25,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Multilobar Pneumonia\n",
      "Prediction: Unknown\n",
      "True Label: Cardiovascular / Pulmonary\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/370 [00:12<21:51,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Abnormal Echocardiogram\n",
      "Prediction: Cardiovascular / Pulmonary\n",
      "True Label: Cardiovascular / Pulmonary\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/370 [00:14<17:19,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: PMT Halo Crown & Vest\n",
      "Prediction: Cardiovascular / Pulmonary\n",
      "True Label: Orthopedic\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|â–         | 5/370 [00:17<18:02,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: MRI Brain and C-T Spine\n",
      "Prediction: Neurology\n",
      "True Label: Neurology\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 6/370 [00:23<22:27,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Status Post T&A\n",
      "Prediction: Cardiovascular / Pulmonary\n",
      "True Label: ENT - Otolaryngology\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 7/370 [00:26<21:43,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: EMG/Nerve Conduction Study\n",
      "Prediction: Neurology\n",
      "True Label: Neurology\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 8/370 [00:29<20:24,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Postop Transanal Excision\n",
      "Prediction: Nephrology\n",
      "True Label: Gastroenterology\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 9/370 [00:32<19:01,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Femoral Artery Cannulation & Aortogram\n",
      "Prediction: Cardiovascular / Pulmonary\n",
      "True Label: Cardiovascular / Pulmonary\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 10/370 [00:34<18:35,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Colonoscopy - 5\n",
      "Prediction: Cardiovascular / Pulmonary\n",
      "True Label: Gastroenterology\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 11/370 [00:38<19:49,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: OB/GYN Consultation - 4\n",
      "Prediction: Cardiovascular / Pulmonary\n",
      "True Label: Obstetrics / Gynecology\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 12/370 [00:41<18:29,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Coronary CT Angiography (CCTA) - 1\n",
      "Prediction: Cardiovascular / Pulmonary\n",
      "True Label: Cardiovascular / Pulmonary\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–Ž         | 13/370 [00:43<16:55,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Thoracotomy & Lobectomy\n",
      "Prediction: Cardiovascular / Pulmonary\n",
      "True Label: Cardiovascular / Pulmonary\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 14/370 [00:46<17:01,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: Ligament Reconstruction & Meniscus Repair\n",
      "Prediction: Neurosurgery\n",
      "True Label: Orthopedic\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 15/370 [00:50<18:45,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: CVA - Discharge Summary\n",
      "Prediction: Cardiovascular / Pulmonary\n",
      "True Label: Cardiovascular / Pulmonary\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load model and tokenizer\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"gemma-lora-medical/checkpoint-1290\",\n",
    "    max_seq_length=512,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv(\"medical_cases_test/medical_cases_test.csv\")\n",
    "test_df = test_df.dropna(subset=[\"sample_name\", \"medical_specialty\"])\n",
    "\n",
    "# Dynamically extract unique medical specialties (target classes)\n",
    "target_classes = sorted(test_df[\"medical_specialty\"].unique())\n",
    "target_classes_lower = [cls.lower() for cls in target_classes]\n",
    "\n",
    "# Predictions\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "\n",
    "print(\"\\n=== Predictions on Test Set ===\\n\")\n",
    "\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    sample_name = test_df.iloc[i][\"sample_name\"]\n",
    "    true_label = test_df.iloc[i][\"medical_specialty\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Classify the following medical sample into one of the following medical specialties:\n",
    "{target_classes}\n",
    "\n",
    "Sample Name: {sample_name}\n",
    "Medical Specialty:\n",
    "\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    prediction_raw = decoded.split(\"Medical Specialty:\")[-1].strip()\n",
    "\n",
    "    # Match prediction to one of the known target classes\n",
    "    matched_class = None\n",
    "    for cls in target_classes:\n",
    "        if cls.lower() in prediction_raw.lower():\n",
    "            matched_class = cls\n",
    "            break\n",
    "\n",
    "    if not matched_class:\n",
    "        for cls in target_classes:\n",
    "            if any(word in prediction_raw.lower() for word in cls.lower().split()):\n",
    "                matched_class = cls\n",
    "                break\n",
    "\n",
    "    if not matched_class:\n",
    "        matched_class = \"Unknown\"\n",
    "\n",
    "    pred_labels.append(matched_class)\n",
    "    true_labels.append(true_label)\n",
    "\n",
    "    print(f\"Sample: {sample_name}\")\n",
    "    print(f\"Prediction: {matched_class}\")\n",
    "    print(f\"True Label: {true_label}\\n\")\n",
    "\n",
    "# Filter out 'Unknown' predictions for evaluation\n",
    "filtered_preds = [p for p in pred_labels if p != \"Unknown\"]\n",
    "filtered_truth = [t for p, t in zip(pred_labels, true_labels) if p != \"Unknown\"]\n",
    "\n",
    "print(\"=== Evaluation Metrics (Excluding 'Unknown') ===\")\n",
    "print(\"Accuracy:\", accuracy_score(filtered_truth, filtered_preds))\n",
    "print(\"Precision:\", precision_score(filtered_truth, filtered_preds, average='macro', zero_division=0))\n",
    "print(\"Recall:\", recall_score(filtered_truth, filtered_preds, average='macro', zero_division=0))\n",
    "print(\"F1 Score:\", f1_score(filtered_truth, filtered_preds, average='macro', zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
