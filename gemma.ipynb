{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TrainingArguments, Trainer, EvalPrediction\n",
    "import torch\n",
    "from peft import LoraConfig\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# LOAD DATA\n",
    "# -------------------------------------------------------------------\n",
    "train_df = pd.read_csv(\"medical_cases_train/medical_cases_train.csv\")\n",
    "val_df = pd.read_csv(\"medical_cases_validation/medical_cases_validation.csv\")\n",
    "test_df = pd.read_csv(\"medical_cases_test/medical_cases_test.csv\")\n",
    "\n",
    "train_set = Dataset.from_pandas(train_df)\n",
    "val_set = Dataset.from_pandas(val_df)\n",
    "test_set = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>transcription</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pacemaker ICD interrogation.  Severe nonischem...</td>\n",
       "      <td>PROCEDURE NOTE: , Pacemaker ICD interrogation....</td>\n",
       "      <td>Pacemaker Interrogation</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>cardiovascular / pulmonary, cardiomyopathy, ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Erythema of the right knee and leg, possible s...</td>\n",
       "      <td>PREOPERATIVE DIAGNOSES: , Erythema of the righ...</td>\n",
       "      <td>Aspiration - Knee Joint</td>\n",
       "      <td>Orthopedic</td>\n",
       "      <td>orthopedic, knee and leg, anterolateral portal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Left cardiac catheterization with selective ri...</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: , Post infarct angina....</td>\n",
       "      <td>Cardiac Cath &amp; Selective Coronary Angiography</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>cardiovascular / pulmonary, selective, angiogr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patient with a history of coronary artery dise...</td>\n",
       "      <td>REASON FOR VISIT: , Acute kidney failure.,HIST...</td>\n",
       "      <td>Acute Kidney Failure</td>\n",
       "      <td>Nephrology</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cardiac evaluation and treatment in a patient ...</td>\n",
       "      <td>REASON FOR REFERRAL: , Cardiac evaluation and ...</td>\n",
       "      <td>Cardiac Consultation - 6</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>Arthroscopy of the left knee, left arthroscopi...</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS:,  Medial meniscal tear...</td>\n",
       "      <td>Arthroscopy, Meniscoplasty, &amp; Chondroplasty</td>\n",
       "      <td>Orthopedic</td>\n",
       "      <td>orthopedic, medial meniscoplasty, arthroscopic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>Normal awake and drowsy (stage I sleep) EEG fo...</td>\n",
       "      <td>DESCRIPTION OF RECORD:  ,This tracing was obta...</td>\n",
       "      <td>Electroencephalogram</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>neurology, gold-plated surface disc electrodes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>MRI of the brain without contrast to evaluate ...</td>\n",
       "      <td>EXAM: , MRI of the brain without contrast.,HIS...</td>\n",
       "      <td>MRI of Brain w/o Contrast.</td>\n",
       "      <td>Neurology</td>\n",
       "      <td>neurology, mri, diffusion, posterior fossa, ax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>The patient comes for three-week postpartum ch...</td>\n",
       "      <td>CHIEF COMPLAINT:,  The patient comes for three...</td>\n",
       "      <td>Three-Week Postpartum Checkup</td>\n",
       "      <td>Obstetrics / Gynecology</td>\n",
       "      <td>obstetrics / gynecology, checkup, allergies, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>Laparoscopic cholecystectomy.  Gallstone pancr...</td>\n",
       "      <td>PREOPERATIVE DIAGNOSIS: ,Gallstone pancreatiti...</td>\n",
       "      <td>Cholecystectomy Laparoscopic</td>\n",
       "      <td>Gastroenterology</td>\n",
       "      <td>gastroenterology, gallstone, gallbladder, panc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1724 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            description  \\\n",
       "0     Pacemaker ICD interrogation.  Severe nonischem...   \n",
       "1     Erythema of the right knee and leg, possible s...   \n",
       "2     Left cardiac catheterization with selective ri...   \n",
       "3     Patient with a history of coronary artery dise...   \n",
       "4     Cardiac evaluation and treatment in a patient ...   \n",
       "...                                                 ...   \n",
       "1719  Arthroscopy of the left knee, left arthroscopi...   \n",
       "1720  Normal awake and drowsy (stage I sleep) EEG fo...   \n",
       "1721  MRI of the brain without contrast to evaluate ...   \n",
       "1722  The patient comes for three-week postpartum ch...   \n",
       "1723  Laparoscopic cholecystectomy.  Gallstone pancr...   \n",
       "\n",
       "                                          transcription  \\\n",
       "0     PROCEDURE NOTE: , Pacemaker ICD interrogation....   \n",
       "1     PREOPERATIVE DIAGNOSES: , Erythema of the righ...   \n",
       "2     PREOPERATIVE DIAGNOSIS: , Post infarct angina....   \n",
       "3     REASON FOR VISIT: , Acute kidney failure.,HIST...   \n",
       "4     REASON FOR REFERRAL: , Cardiac evaluation and ...   \n",
       "...                                                 ...   \n",
       "1719  PREOPERATIVE DIAGNOSIS:,  Medial meniscal tear...   \n",
       "1720  DESCRIPTION OF RECORD:  ,This tracing was obta...   \n",
       "1721  EXAM: , MRI of the brain without contrast.,HIS...   \n",
       "1722  CHIEF COMPLAINT:,  The patient comes for three...   \n",
       "1723  PREOPERATIVE DIAGNOSIS: ,Gallstone pancreatiti...   \n",
       "\n",
       "                                        sample_name  \\\n",
       "0                           Pacemaker Interrogation   \n",
       "1                           Aspiration - Knee Joint   \n",
       "2     Cardiac Cath & Selective Coronary Angiography   \n",
       "3                              Acute Kidney Failure   \n",
       "4                          Cardiac Consultation - 6   \n",
       "...                                             ...   \n",
       "1719    Arthroscopy, Meniscoplasty, & Chondroplasty   \n",
       "1720                           Electroencephalogram   \n",
       "1721                     MRI of Brain w/o Contrast.   \n",
       "1722                  Three-Week Postpartum Checkup   \n",
       "1723                   Cholecystectomy Laparoscopic   \n",
       "\n",
       "               medical_specialty  \\\n",
       "0     Cardiovascular / Pulmonary   \n",
       "1                     Orthopedic   \n",
       "2     Cardiovascular / Pulmonary   \n",
       "3                     Nephrology   \n",
       "4     Cardiovascular / Pulmonary   \n",
       "...                          ...   \n",
       "1719                  Orthopedic   \n",
       "1720                   Neurology   \n",
       "1721                   Neurology   \n",
       "1722     Obstetrics / Gynecology   \n",
       "1723            Gastroenterology   \n",
       "\n",
       "                                               keywords  \n",
       "0     cardiovascular / pulmonary, cardiomyopathy, ve...  \n",
       "1     orthopedic, knee and leg, anterolateral portal...  \n",
       "2     cardiovascular / pulmonary, selective, angiogr...  \n",
       "3                                                  None  \n",
       "4                                                  None  \n",
       "...                                                 ...  \n",
       "1719  orthopedic, medial meniscoplasty, arthroscopic...  \n",
       "1720  neurology, gold-plated surface disc electrodes...  \n",
       "1721  neurology, mri, diffusion, posterior fossa, ax...  \n",
       "1722  obstetrics / gynecology, checkup, allergies, p...  \n",
       "1723  gastroenterology, gallstone, gallbladder, panc...  \n",
       "\n",
       "[1724 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5a9c6dc35a477da353204cd282fdfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b201a5cb13524d60be3c455249293816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed670e6b15df4f0d85d5f3d9c6c8a69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.51.1.\n",
      "   \\\\   /|    NVIDIA RTX A2000 12GB. Num GPUs = 1. Max memory: 11.757 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b2b77d4ee84d7ca642a56317b59a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1724 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43cc0228cc24699aa7ac6139e818bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee8118eec174373b293e51418679832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/370 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_680992/3143608032.py:80: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,724 | Num Epochs = 5 | Total steps = 1,075\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 745,472/1,000,000,000 (0.07% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1075' max='1075' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1075/1075 1:18:30, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.470900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.374000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.048200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./gemma-lora-medical/tokenizer_config.json',\n",
       " './gemma-lora-medical/special_tokens_map.json',\n",
       " './gemma-lora-medical/tokenizer.model',\n",
       " './gemma-lora-medical/added_tokens.json',\n",
       " './gemma-lora-medical/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# FORMAT PROMPTS\n",
    "# -------------------------------------------------------------------\n",
    "def format_prompt(example):\n",
    "    return {\n",
    "        \"text\": f\"<start_of_turn>user\\nDescription:{example['description']}<end_of_turn> \\\n",
    "        \\n<start_of_turn>model\\n{example['medical_specialty']}<end_of_turn>\"\n",
    "    }\n",
    "\n",
    "train_dataset = train_set.map(format_prompt)\n",
    "val_dataset = val_set.map(format_prompt)\n",
    "test_dataset = test_set.map(format_prompt)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# LOAD MODEL\n",
    "# -------------------------------------------------------------------\n",
    "model_name = \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=512,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# APPLY LoRA\n",
    "# -------------------------------------------------------------------\n",
    "FastLanguageModel.for_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model.add_adapter(lora_config)\n",
    "\n",
    "\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = tokens[\"input_ids\"].copy()\n",
    "\n",
    "    # Mask out user prompt part with -100\n",
    "    user_end = tokens[\"input_ids\"].index(tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")) + 1\n",
    "    labels[:user_end] = [-100] * user_end\n",
    "\n",
    "    tokens[\"labels\"] = labels\n",
    "    return tokens\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(tokenize, remove_columns=val_dataset.column_names)\n",
    "test_dataset = test_dataset.map(tokenize, remove_columns=test_dataset.column_names)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# TRAINING ARGUMENTS\n",
    "# -------------------------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-lora-medical\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# TRAINER\n",
    "# -------------------------------------------------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# SAVE MODEL\n",
    "# -------------------------------------------------------------------\n",
    "model.save_pretrained(\"./gemma-lora-medical\")\n",
    "tokenizer.save_pretrained(\"./gemma-lora-medical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Predictions on Test Set ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 370/370 [01:45<00:00,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Metrics (Excluding 'Unknown') ===\n",
      "Total predictions: 370\n",
      "Unknown predictions: 62\n",
      "Accuracy: 0.5032467532467533\n",
      "Precision: 0.48452737286633446\n",
      "Recall: 0.417607763409892\n",
      "F1 Score: 0.3908247963399516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------------------------\n",
    "# SETUP\n",
    "# -------------------------------------------------------------------\n",
    "target_classes = sorted(np.unique(test_df[\"medical_specialty\"]))\n",
    "target_classes_str = \"\\n\".join(target_classes)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "y_pt = []\n",
    "y_gt = []\n",
    "\n",
    "# Clear logs\n",
    "open(\"gemma.txt\", \"w\").close()\n",
    "open(\"gemma_unknown.txt\", \"w\").close()\n",
    "\n",
    "print(\"\\n=== Predictions on Test Set ===\\n\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# MATCHING FUNCTION\n",
    "# -------------------------------------------------------------------\n",
    "def match_class(prediction_raw, target_classes):\n",
    "    pred = prediction_raw.lower().strip()\n",
    "\n",
    "    # Exact match\n",
    "    for cls in target_classes:\n",
    "        if pred == cls.lower():\n",
    "            return cls\n",
    "\n",
    "    # Substring match\n",
    "    for cls in target_classes:\n",
    "        if cls.lower() in pred:\n",
    "            return cls\n",
    "\n",
    "    # Word overlap\n",
    "    pred_words = set(pred.split())\n",
    "    for cls in target_classes:\n",
    "        cls_words = set(cls.lower().split())\n",
    "        if pred_words & cls_words:\n",
    "            return cls\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# INFERENCE LOOP\n",
    "# -------------------------------------------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    true_label = test_df.iloc[i][\"medical_specialty\"]\n",
    "    description = test_df.iloc[i][\"description\"]\n",
    "\n",
    "    prompt = f\"\"\"Classify the following medical case description into one of the following medical specialties.\n",
    "\n",
    "Respond with only the name of the specialty. One-word answer. No explanations.\n",
    "\n",
    "Choices:\n",
    "{target_classes_str}\n",
    "\n",
    "Description:\n",
    "{description}\n",
    "\n",
    "Medical Specialty:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    prediction_raw = decoded.split(\"Medical Specialty:\")[-1].strip()\n",
    "\n",
    "    matched_class = match_class(prediction_raw, target_classes)\n",
    "\n",
    "    if matched_class == \"Unknown\":\n",
    "        with open(\"gemma_unknown.txt\", \"a\") as f:\n",
    "            f.write(f\"[Unknown] Raw prediction: {prediction_raw}\\nDescription: {description}\\n\\n\")\n",
    "\n",
    "    y_pt.append(matched_class)\n",
    "    y_gt.append(true_label)\n",
    "\n",
    "    with open(\"gemma.txt\", \"a\") as f:\n",
    "        f.write(f\"Prediction: {matched_class}\\n\")\n",
    "        f.write(f\"True Label: {true_label}\\n\\n\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# EVALUATION\n",
    "# -------------------------------------------------------------------\n",
    "filtered_preds = [p for p in y_pt if p != \"Unknown\"]\n",
    "filtered_truth = [t for p, t in zip(y_pt, y_gt) if p != \"Unknown\"]\n",
    "\n",
    "print(\"\\n=== Evaluation Metrics (Excluding 'Unknown') ===\")\n",
    "print(f\"Total predictions: {len(y_pt)}\")\n",
    "print(f\"Unknown predictions: {y_pt.count('Unknown')}\")\n",
    "print(\"Accuracy:\", accuracy_score(filtered_truth, filtered_preds))\n",
    "print(\"Precision:\", precision_score(filtered_truth, filtered_preds, average='macro', zero_division=0))\n",
    "print(\"Recall:\", recall_score(filtered_truth, filtered_preds, average='macro', zero_division=0))\n",
    "print(\"F1 Score:\", f1_score(filtered_truth, filtered_preds, average='macro', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== Predictions on Test Set ===\n",
    "\n",
    "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 370/370 [01:45<00:00,  3.50it/s]\n",
    "\n",
    "=== Evaluation Metrics (Excluding 'Unknown') ===\n",
    "\n",
    "Total predictions: 370\n",
    "\n",
    "Unknown predictions: 62\n",
    "\n",
    "Accuracy: 0.5032467532467533\n",
    "\n",
    "Precision: 0.48452737286633446\n",
    "\n",
    "Recall: 0.417607763409892\n",
    "\n",
    "F1 Score: 0.3908247963399516"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ayush",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
