{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.974477958236659,
  "eval_steps": 500,
  "global_step": 1290,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04640371229698376,
      "grad_norm": 43.33512878417969,
      "learning_rate": 0.00014,
      "loss": 6.1168,
      "step": 10
    },
    {
      "epoch": 0.09280742459396751,
      "grad_norm": 0.6514455080032349,
      "learning_rate": 0.00019890625,
      "loss": 1.0872,
      "step": 20
    },
    {
      "epoch": 0.13921113689095127,
      "grad_norm": 0.5214906930923462,
      "learning_rate": 0.00019734375,
      "loss": 0.6106,
      "step": 30
    },
    {
      "epoch": 0.18561484918793503,
      "grad_norm": 0.4515000581741333,
      "learning_rate": 0.00019578125,
      "loss": 0.5011,
      "step": 40
    },
    {
      "epoch": 0.23201856148491878,
      "grad_norm": 0.34976616501808167,
      "learning_rate": 0.00019421875,
      "loss": 0.3799,
      "step": 50
    },
    {
      "epoch": 0.27842227378190254,
      "grad_norm": 0.34051722288131714,
      "learning_rate": 0.00019265625,
      "loss": 0.3523,
      "step": 60
    },
    {
      "epoch": 0.3248259860788863,
      "grad_norm": 0.26263219118118286,
      "learning_rate": 0.00019109375,
      "loss": 0.2905,
      "step": 70
    },
    {
      "epoch": 0.37122969837587005,
      "grad_norm": 0.23759496212005615,
      "learning_rate": 0.00018953125,
      "loss": 0.2688,
      "step": 80
    },
    {
      "epoch": 0.4176334106728538,
      "grad_norm": 0.3013644218444824,
      "learning_rate": 0.00018796875,
      "loss": 0.266,
      "step": 90
    },
    {
      "epoch": 0.46403712296983757,
      "grad_norm": 0.2753293812274933,
      "learning_rate": 0.00018640625,
      "loss": 0.2913,
      "step": 100
    },
    {
      "epoch": 0.5104408352668214,
      "grad_norm": 0.2715674042701721,
      "learning_rate": 0.00018484375,
      "loss": 0.2586,
      "step": 110
    },
    {
      "epoch": 0.5568445475638051,
      "grad_norm": 0.2650272846221924,
      "learning_rate": 0.00018328125,
      "loss": 0.2542,
      "step": 120
    },
    {
      "epoch": 0.6032482598607889,
      "grad_norm": 0.23527799546718597,
      "learning_rate": 0.00018171875,
      "loss": 0.229,
      "step": 130
    },
    {
      "epoch": 0.6496519721577726,
      "grad_norm": 0.26171374320983887,
      "learning_rate": 0.00018015625,
      "loss": 0.2869,
      "step": 140
    },
    {
      "epoch": 0.6960556844547564,
      "grad_norm": 0.31242483854293823,
      "learning_rate": 0.00017859375000000001,
      "loss": 0.2438,
      "step": 150
    },
    {
      "epoch": 0.7424593967517401,
      "grad_norm": 0.34858110547065735,
      "learning_rate": 0.00017703125,
      "loss": 0.2705,
      "step": 160
    },
    {
      "epoch": 0.7888631090487239,
      "grad_norm": 0.24688534438610077,
      "learning_rate": 0.00017546875,
      "loss": 0.2549,
      "step": 170
    },
    {
      "epoch": 0.8352668213457076,
      "grad_norm": 0.2848723530769348,
      "learning_rate": 0.00017390625000000002,
      "loss": 0.2389,
      "step": 180
    },
    {
      "epoch": 0.8816705336426914,
      "grad_norm": 0.31952905654907227,
      "learning_rate": 0.00017234375,
      "loss": 0.241,
      "step": 190
    },
    {
      "epoch": 0.9280742459396751,
      "grad_norm": 0.28527212142944336,
      "learning_rate": 0.00017078125,
      "loss": 0.2446,
      "step": 200
    },
    {
      "epoch": 0.974477958236659,
      "grad_norm": 0.2643864154815674,
      "learning_rate": 0.00016921875000000002,
      "loss": 0.231,
      "step": 210
    },
    {
      "epoch": 1.0185614849187936,
      "grad_norm": 0.32232704758644104,
      "learning_rate": 0.00016765625,
      "loss": 0.2486,
      "step": 220
    },
    {
      "epoch": 1.0649651972157772,
      "grad_norm": 0.35998857021331787,
      "learning_rate": 0.00016609375,
      "loss": 0.2496,
      "step": 230
    },
    {
      "epoch": 1.111368909512761,
      "grad_norm": 0.2781033515930176,
      "learning_rate": 0.00016453125,
      "loss": 0.242,
      "step": 240
    },
    {
      "epoch": 1.1577726218097448,
      "grad_norm": 0.30591002106666565,
      "learning_rate": 0.00016296875,
      "loss": 0.2296,
      "step": 250
    },
    {
      "epoch": 1.2041763341067284,
      "grad_norm": 0.2758937180042267,
      "learning_rate": 0.00016140625,
      "loss": 0.2574,
      "step": 260
    },
    {
      "epoch": 1.2505800464037122,
      "grad_norm": 0.19192998111248016,
      "learning_rate": 0.00015984375,
      "loss": 0.2177,
      "step": 270
    },
    {
      "epoch": 1.296983758700696,
      "grad_norm": 0.2548134922981262,
      "learning_rate": 0.00015828125,
      "loss": 0.2187,
      "step": 280
    },
    {
      "epoch": 1.3433874709976799,
      "grad_norm": 0.24208778142929077,
      "learning_rate": 0.00015671875,
      "loss": 0.2494,
      "step": 290
    },
    {
      "epoch": 1.3897911832946637,
      "grad_norm": 0.24728715419769287,
      "learning_rate": 0.00015515625,
      "loss": 0.2389,
      "step": 300
    },
    {
      "epoch": 1.4361948955916473,
      "grad_norm": 0.24101310968399048,
      "learning_rate": 0.00015359375,
      "loss": 0.2168,
      "step": 310
    },
    {
      "epoch": 1.482598607888631,
      "grad_norm": 0.2604079842567444,
      "learning_rate": 0.00015203125,
      "loss": 0.2162,
      "step": 320
    },
    {
      "epoch": 1.5290023201856149,
      "grad_norm": 0.24003276228904724,
      "learning_rate": 0.00015046875,
      "loss": 0.2122,
      "step": 330
    },
    {
      "epoch": 1.5754060324825985,
      "grad_norm": 0.3590206205844879,
      "learning_rate": 0.00014890625,
      "loss": 0.2358,
      "step": 340
    },
    {
      "epoch": 1.6218097447795823,
      "grad_norm": 0.2872692942619324,
      "learning_rate": 0.00014734375000000001,
      "loss": 0.2245,
      "step": 350
    },
    {
      "epoch": 1.668213457076566,
      "grad_norm": 0.27527329325675964,
      "learning_rate": 0.00014578125,
      "loss": 0.2053,
      "step": 360
    },
    {
      "epoch": 1.71461716937355,
      "grad_norm": 0.24902689456939697,
      "learning_rate": 0.00014421875,
      "loss": 0.2147,
      "step": 370
    },
    {
      "epoch": 1.7610208816705337,
      "grad_norm": 0.27806535363197327,
      "learning_rate": 0.00014265625000000002,
      "loss": 0.2372,
      "step": 380
    },
    {
      "epoch": 1.8074245939675175,
      "grad_norm": 0.23227979242801666,
      "learning_rate": 0.00014109375,
      "loss": 0.23,
      "step": 390
    },
    {
      "epoch": 1.8538283062645011,
      "grad_norm": 0.2678009569644928,
      "learning_rate": 0.00013953125,
      "loss": 0.2461,
      "step": 400
    },
    {
      "epoch": 1.900232018561485,
      "grad_norm": 0.2775987684726715,
      "learning_rate": 0.00013796875,
      "loss": 0.2116,
      "step": 410
    },
    {
      "epoch": 1.9466357308584685,
      "grad_norm": 0.2730591297149658,
      "learning_rate": 0.00013640625,
      "loss": 0.2198,
      "step": 420
    },
    {
      "epoch": 1.9930394431554523,
      "grad_norm": 0.26028603315353394,
      "learning_rate": 0.00013484375,
      "loss": 0.2151,
      "step": 430
    },
    {
      "epoch": 2.0371229698375872,
      "grad_norm": 0.29578691720962524,
      "learning_rate": 0.00013328125,
      "loss": 0.1897,
      "step": 440
    },
    {
      "epoch": 2.0835266821345706,
      "grad_norm": 0.26612645387649536,
      "learning_rate": 0.00013171875,
      "loss": 0.2118,
      "step": 450
    },
    {
      "epoch": 2.1299303944315544,
      "grad_norm": 0.2884550094604492,
      "learning_rate": 0.00013015625,
      "loss": 0.2506,
      "step": 460
    },
    {
      "epoch": 2.176334106728538,
      "grad_norm": 0.27066928148269653,
      "learning_rate": 0.00012859375,
      "loss": 0.2103,
      "step": 470
    },
    {
      "epoch": 2.222737819025522,
      "grad_norm": 0.24607840180397034,
      "learning_rate": 0.00012703125,
      "loss": 0.1999,
      "step": 480
    },
    {
      "epoch": 2.269141531322506,
      "grad_norm": 0.2413349747657776,
      "learning_rate": 0.00012546875,
      "loss": 0.2194,
      "step": 490
    },
    {
      "epoch": 2.3155452436194897,
      "grad_norm": 0.28820860385894775,
      "learning_rate": 0.00012390625,
      "loss": 0.2182,
      "step": 500
    },
    {
      "epoch": 2.3619489559164735,
      "grad_norm": 0.30668607354164124,
      "learning_rate": 0.00012234375,
      "loss": 0.2171,
      "step": 510
    },
    {
      "epoch": 2.408352668213457,
      "grad_norm": 0.23955240845680237,
      "learning_rate": 0.00012078125,
      "loss": 0.2041,
      "step": 520
    },
    {
      "epoch": 2.4547563805104406,
      "grad_norm": 0.31858742237091064,
      "learning_rate": 0.00011921875000000001,
      "loss": 0.2413,
      "step": 530
    },
    {
      "epoch": 2.5011600928074245,
      "grad_norm": 0.2621336281299591,
      "learning_rate": 0.00011765625,
      "loss": 0.2049,
      "step": 540
    },
    {
      "epoch": 2.5475638051044083,
      "grad_norm": 0.2297603338956833,
      "learning_rate": 0.00011609375,
      "loss": 0.2268,
      "step": 550
    },
    {
      "epoch": 2.593967517401392,
      "grad_norm": 0.2661129832267761,
      "learning_rate": 0.00011453125,
      "loss": 0.2204,
      "step": 560
    },
    {
      "epoch": 2.640371229698376,
      "grad_norm": 0.2704285681247711,
      "learning_rate": 0.00011296875,
      "loss": 0.2406,
      "step": 570
    },
    {
      "epoch": 2.6867749419953597,
      "grad_norm": 0.29159221053123474,
      "learning_rate": 0.00011140625,
      "loss": 0.2337,
      "step": 580
    },
    {
      "epoch": 2.7331786542923435,
      "grad_norm": 0.2639252245426178,
      "learning_rate": 0.00010984375,
      "loss": 0.1959,
      "step": 590
    },
    {
      "epoch": 2.7795823665893273,
      "grad_norm": 0.24529321491718292,
      "learning_rate": 0.00010828125,
      "loss": 0.2107,
      "step": 600
    },
    {
      "epoch": 2.825986078886311,
      "grad_norm": 0.2780901789665222,
      "learning_rate": 0.00010671875,
      "loss": 0.2139,
      "step": 610
    },
    {
      "epoch": 2.8723897911832945,
      "grad_norm": 0.27419087290763855,
      "learning_rate": 0.00010515625,
      "loss": 0.1992,
      "step": 620
    },
    {
      "epoch": 2.9187935034802783,
      "grad_norm": 0.22937488555908203,
      "learning_rate": 0.00010359375,
      "loss": 0.1901,
      "step": 630
    },
    {
      "epoch": 2.965197215777262,
      "grad_norm": 0.287356972694397,
      "learning_rate": 0.00010203125,
      "loss": 0.2169,
      "step": 640
    },
    {
      "epoch": 3.0092807424593966,
      "grad_norm": 0.26562613248825073,
      "learning_rate": 0.00010046875,
      "loss": 0.227,
      "step": 650
    },
    {
      "epoch": 3.0556844547563804,
      "grad_norm": 0.31714171171188354,
      "learning_rate": 9.890625e-05,
      "loss": 0.1954,
      "step": 660
    },
    {
      "epoch": 3.102088167053364,
      "grad_norm": 0.2763700485229492,
      "learning_rate": 9.734375e-05,
      "loss": 0.2003,
      "step": 670
    },
    {
      "epoch": 3.148491879350348,
      "grad_norm": 0.3389637768268585,
      "learning_rate": 9.578125e-05,
      "loss": 0.2265,
      "step": 680
    },
    {
      "epoch": 3.194895591647332,
      "grad_norm": 0.2622736692428589,
      "learning_rate": 9.421875e-05,
      "loss": 0.2157,
      "step": 690
    },
    {
      "epoch": 3.2412993039443156,
      "grad_norm": 0.29047051072120667,
      "learning_rate": 9.265625e-05,
      "loss": 0.2282,
      "step": 700
    },
    {
      "epoch": 3.2877030162412995,
      "grad_norm": 0.29179447889328003,
      "learning_rate": 9.109375e-05,
      "loss": 0.1877,
      "step": 710
    },
    {
      "epoch": 3.3341067285382833,
      "grad_norm": 0.21358025074005127,
      "learning_rate": 8.953125e-05,
      "loss": 0.2099,
      "step": 720
    },
    {
      "epoch": 3.3805104408352666,
      "grad_norm": 0.25922465324401855,
      "learning_rate": 8.796875e-05,
      "loss": 0.2077,
      "step": 730
    },
    {
      "epoch": 3.4269141531322505,
      "grad_norm": 0.24299566447734833,
      "learning_rate": 8.640625e-05,
      "loss": 0.201,
      "step": 740
    },
    {
      "epoch": 3.4733178654292343,
      "grad_norm": 0.3146190643310547,
      "learning_rate": 8.484375e-05,
      "loss": 0.2123,
      "step": 750
    },
    {
      "epoch": 3.519721577726218,
      "grad_norm": 0.30945685505867004,
      "learning_rate": 8.328125e-05,
      "loss": 0.2065,
      "step": 760
    },
    {
      "epoch": 3.566125290023202,
      "grad_norm": 0.250272274017334,
      "learning_rate": 8.171875e-05,
      "loss": 0.2013,
      "step": 770
    },
    {
      "epoch": 3.6125290023201857,
      "grad_norm": 0.2913254201412201,
      "learning_rate": 8.015625e-05,
      "loss": 0.2091,
      "step": 780
    },
    {
      "epoch": 3.6589327146171695,
      "grad_norm": 0.32388952374458313,
      "learning_rate": 7.859375e-05,
      "loss": 0.2121,
      "step": 790
    },
    {
      "epoch": 3.705336426914153,
      "grad_norm": 0.2904292941093445,
      "learning_rate": 7.703125e-05,
      "loss": 0.1869,
      "step": 800
    },
    {
      "epoch": 3.7517401392111367,
      "grad_norm": 0.30612248182296753,
      "learning_rate": 7.546875e-05,
      "loss": 0.1971,
      "step": 810
    },
    {
      "epoch": 3.7981438515081205,
      "grad_norm": 0.2548988163471222,
      "learning_rate": 7.390625e-05,
      "loss": 0.2055,
      "step": 820
    },
    {
      "epoch": 3.8445475638051043,
      "grad_norm": 0.24423205852508545,
      "learning_rate": 7.234375e-05,
      "loss": 0.2015,
      "step": 830
    },
    {
      "epoch": 3.890951276102088,
      "grad_norm": 0.31059691309928894,
      "learning_rate": 7.078125e-05,
      "loss": 0.2132,
      "step": 840
    },
    {
      "epoch": 3.937354988399072,
      "grad_norm": 0.2797725796699524,
      "learning_rate": 6.921875e-05,
      "loss": 0.2109,
      "step": 850
    },
    {
      "epoch": 3.9837587006960558,
      "grad_norm": 0.301550030708313,
      "learning_rate": 6.765625e-05,
      "loss": 0.2159,
      "step": 860
    },
    {
      "epoch": 4.027842227378191,
      "grad_norm": 0.25840163230895996,
      "learning_rate": 6.609375e-05,
      "loss": 0.1868,
      "step": 870
    },
    {
      "epoch": 4.0742459396751745,
      "grad_norm": 0.3555222749710083,
      "learning_rate": 6.453125e-05,
      "loss": 0.1968,
      "step": 880
    },
    {
      "epoch": 4.120649651972157,
      "grad_norm": 0.2650313675403595,
      "learning_rate": 6.296875e-05,
      "loss": 0.2165,
      "step": 890
    },
    {
      "epoch": 4.167053364269141,
      "grad_norm": 0.26801326870918274,
      "learning_rate": 6.140625e-05,
      "loss": 0.2048,
      "step": 900
    },
    {
      "epoch": 4.213457076566125,
      "grad_norm": 0.3155733346939087,
      "learning_rate": 5.984375e-05,
      "loss": 0.1834,
      "step": 910
    },
    {
      "epoch": 4.259860788863109,
      "grad_norm": 0.29455795884132385,
      "learning_rate": 5.828125e-05,
      "loss": 0.1999,
      "step": 920
    },
    {
      "epoch": 4.306264501160093,
      "grad_norm": 0.30548134446144104,
      "learning_rate": 5.671875e-05,
      "loss": 0.2121,
      "step": 930
    },
    {
      "epoch": 4.352668213457076,
      "grad_norm": 0.3342978060245514,
      "learning_rate": 5.5156249999999996e-05,
      "loss": 0.1993,
      "step": 940
    },
    {
      "epoch": 4.39907192575406,
      "grad_norm": 0.3107120394706726,
      "learning_rate": 5.359375e-05,
      "loss": 0.2046,
      "step": 950
    },
    {
      "epoch": 4.445475638051044,
      "grad_norm": 0.30182209610939026,
      "learning_rate": 5.203125e-05,
      "loss": 0.2164,
      "step": 960
    },
    {
      "epoch": 4.491879350348028,
      "grad_norm": 0.3405841886997223,
      "learning_rate": 5.046875e-05,
      "loss": 0.2165,
      "step": 970
    },
    {
      "epoch": 4.538283062645012,
      "grad_norm": 0.36137232184410095,
      "learning_rate": 4.8906250000000006e-05,
      "loss": 0.2104,
      "step": 980
    },
    {
      "epoch": 4.5846867749419955,
      "grad_norm": 0.32687586545944214,
      "learning_rate": 4.734375e-05,
      "loss": 0.1856,
      "step": 990
    },
    {
      "epoch": 4.631090487238979,
      "grad_norm": 0.27378058433532715,
      "learning_rate": 4.5781250000000005e-05,
      "loss": 0.228,
      "step": 1000
    },
    {
      "epoch": 4.677494199535963,
      "grad_norm": 0.3101023733615875,
      "learning_rate": 4.421875e-05,
      "loss": 0.2135,
      "step": 1010
    },
    {
      "epoch": 4.723897911832947,
      "grad_norm": 0.29462942481040955,
      "learning_rate": 4.2656250000000003e-05,
      "loss": 0.1665,
      "step": 1020
    },
    {
      "epoch": 4.770301624129931,
      "grad_norm": 0.3099161684513092,
      "learning_rate": 4.1093750000000006e-05,
      "loss": 0.2005,
      "step": 1030
    },
    {
      "epoch": 4.816705336426914,
      "grad_norm": 0.25542283058166504,
      "learning_rate": 3.953125e-05,
      "loss": 0.1858,
      "step": 1040
    },
    {
      "epoch": 4.8631090487238975,
      "grad_norm": 0.29496827721595764,
      "learning_rate": 3.7968750000000005e-05,
      "loss": 0.2035,
      "step": 1050
    },
    {
      "epoch": 4.909512761020881,
      "grad_norm": 0.27078962326049805,
      "learning_rate": 3.640625e-05,
      "loss": 0.2034,
      "step": 1060
    },
    {
      "epoch": 4.955916473317865,
      "grad_norm": 0.27191659808158875,
      "learning_rate": 3.484375e-05,
      "loss": 0.1906,
      "step": 1070
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.404334157705307,
      "learning_rate": 3.3281250000000006e-05,
      "loss": 0.2058,
      "step": 1080
    },
    {
      "epoch": 5.046403712296984,
      "grad_norm": 0.3193175196647644,
      "learning_rate": 3.171875e-05,
      "loss": 0.2056,
      "step": 1090
    },
    {
      "epoch": 5.092807424593968,
      "grad_norm": 0.3311578035354614,
      "learning_rate": 3.015625e-05,
      "loss": 0.2144,
      "step": 1100
    },
    {
      "epoch": 5.139211136890951,
      "grad_norm": 0.27768275141716003,
      "learning_rate": 2.8593750000000004e-05,
      "loss": 0.1736,
      "step": 1110
    },
    {
      "epoch": 5.185614849187935,
      "grad_norm": 0.27759695053100586,
      "learning_rate": 2.7031250000000003e-05,
      "loss": 0.1961,
      "step": 1120
    },
    {
      "epoch": 5.232018561484919,
      "grad_norm": 0.2959957420825958,
      "learning_rate": 2.5468750000000002e-05,
      "loss": 0.1919,
      "step": 1130
    },
    {
      "epoch": 5.278422273781903,
      "grad_norm": 0.2894943654537201,
      "learning_rate": 2.3906250000000002e-05,
      "loss": 0.1597,
      "step": 1140
    },
    {
      "epoch": 5.324825986078887,
      "grad_norm": 0.4077847898006439,
      "learning_rate": 2.234375e-05,
      "loss": 0.1947,
      "step": 1150
    },
    {
      "epoch": 5.3712296983758705,
      "grad_norm": 0.33386287093162537,
      "learning_rate": 2.0781250000000004e-05,
      "loss": 0.1874,
      "step": 1160
    },
    {
      "epoch": 5.417633410672853,
      "grad_norm": 0.27952513098716736,
      "learning_rate": 1.9218750000000003e-05,
      "loss": 0.2046,
      "step": 1170
    },
    {
      "epoch": 5.464037122969837,
      "grad_norm": 0.36839789152145386,
      "learning_rate": 1.7656250000000002e-05,
      "loss": 0.1943,
      "step": 1180
    },
    {
      "epoch": 5.510440835266821,
      "grad_norm": 0.3031448423862457,
      "learning_rate": 1.609375e-05,
      "loss": 0.1998,
      "step": 1190
    },
    {
      "epoch": 5.556844547563805,
      "grad_norm": 0.3343026041984558,
      "learning_rate": 1.4531250000000003e-05,
      "loss": 0.1842,
      "step": 1200
    },
    {
      "epoch": 5.603248259860789,
      "grad_norm": 0.33616742491722107,
      "learning_rate": 1.2968750000000002e-05,
      "loss": 0.2265,
      "step": 1210
    },
    {
      "epoch": 5.6496519721577725,
      "grad_norm": 0.3527214825153351,
      "learning_rate": 1.140625e-05,
      "loss": 0.2179,
      "step": 1220
    },
    {
      "epoch": 5.696055684454756,
      "grad_norm": 0.3487522006034851,
      "learning_rate": 9.84375e-06,
      "loss": 0.1812,
      "step": 1230
    },
    {
      "epoch": 5.74245939675174,
      "grad_norm": 0.31809625029563904,
      "learning_rate": 8.28125e-06,
      "loss": 0.1924,
      "step": 1240
    },
    {
      "epoch": 5.788863109048724,
      "grad_norm": 0.30179089307785034,
      "learning_rate": 6.71875e-06,
      "loss": 0.1942,
      "step": 1250
    },
    {
      "epoch": 5.835266821345708,
      "grad_norm": 0.28049325942993164,
      "learning_rate": 5.15625e-06,
      "loss": 0.1932,
      "step": 1260
    },
    {
      "epoch": 5.8816705336426915,
      "grad_norm": 0.29756268858909607,
      "learning_rate": 3.5937499999999997e-06,
      "loss": 0.2208,
      "step": 1270
    },
    {
      "epoch": 5.928074245939675,
      "grad_norm": 0.3835432827472687,
      "learning_rate": 2.0312500000000002e-06,
      "loss": 0.1997,
      "step": 1280
    },
    {
      "epoch": 5.974477958236659,
      "grad_norm": 0.2762139141559601,
      "learning_rate": 4.6875e-07,
      "loss": 0.1966,
      "step": 1290
    }
  ],
  "logging_steps": 10,
  "max_steps": 1290,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.8880065183744e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
